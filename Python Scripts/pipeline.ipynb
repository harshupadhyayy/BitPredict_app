{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kmPAEoODh2ww",
        "zdpR6kDGuaOS",
        "8b75RW9sh4Mq",
        "AqPvIH9mqz08",
        "dXLL7LhU6FIE",
        "VjhK8MlHPFSi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmPAEoODh2ww"
      },
      "source": [
        "\n",
        "### **Reddit Segment** \n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4KKMxIoiC59"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXr9ELBciv9O"
      },
      "source": [
        "# def get_pushshift_data(query, after, before, sub):\n",
        "def get_pushshift_data(query, after, sub):\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?title=' + str(query) + '&size=1000&after=' + str(\n",
        "        after) + '&subreddit=' + str(sub)\n",
        "    print(url)\n",
        "    r = requests.get(url)\n",
        "    data = json.loads(r.text)\n",
        "    return data['data']\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFrlj_2bi3A3"
      },
      "source": [
        "def collect_sub_data(subm):\n",
        "    sub_data = list()  # list to store data points\n",
        "    title = subm['title']\n",
        "    url = subm['url']\n",
        "    try:\n",
        "        # if flair is available then get it, else set 'NaN'\n",
        "        flair = subm['link_flair_text']\n",
        "    except KeyError:\n",
        "        flair = 'NaN'\n",
        "    author = subm['author']\n",
        "    sub_id = subm['id']\n",
        "    score = subm['score']\n",
        "    try:\n",
        "        # if selftext is available then get it, else set it empty\n",
        "        selftext = subm['selftext']\n",
        "        list_of_empty_markers = ['[removed]', '[deleted]']\n",
        "        # many times selftext would be removed or deleted, if thats the case then set it empty\n",
        "        if selftext in list_of_empty_markers:\n",
        "            selftext = ''\n",
        "    except:\n",
        "        selftext = ''\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc'])  # 1520561700.0\n",
        "    numComms = subm['num_comments']\n",
        "    permalink = subm['permalink']\n",
        "\n",
        "    sub_data.append((sub_id, title, selftext, url, author, score, created, numComms, permalink, flair))\n",
        "    sub_stats[sub_id] = sub_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKCwdiB6jAmA"
      },
      "source": [
        "def write_subs_to_file():\n",
        "    \n",
        "    upload_count = 0\n",
        "    \n",
        "    columns = ['post_id', 'title', 'selftext', 'url', 'author', 'score', 'publish_date', 'num_of_comments','permalink', 'flair']\n",
        "    # print(\"Ankur\")\n",
        "    df_reddit1 = pd.DataFrame(columns = columns)\n",
        "    \n",
        "\n",
        "    for sub in sub_stats:\n",
        "      li =list(sub_stats[sub][0])\n",
        "      # print(\"Shape\",type(li),li[0])\n",
        "      # df_reddit2 = pd.DataFrame(li,columns=['post_id', 'title', 'selftext', 'url', 'author', 'score', 'publish_date', 'num_of_comments','permalink', 'flair'])\n",
        "      # print(df_reddit2)\n",
        "      # df_reddit_1.append(df_reddit2,ignore_index = True)\n",
        "      \n",
        "      # df_reddit1.iloc[upload_count] = li\n",
        "      \n",
        "      # print(type(li),len(li),li)\n",
        "      # print(df_reddit1)\n",
        "\n",
        "      di1 = dict.fromkeys(columns,'')\n",
        "      di1['post_id'],di1['title'],di1['selftext'],di1['url'],di1['author'],di1['score'],di1['publish_date'],di1['num_of_comments'],di1['permalink'],di1['flair'] = li[0],li[1],li[2],li[3],li[4],li[5],li[6],li[7],li[8],li[9]\n",
        "      # print(di1)\n",
        "      df_reddit1 = df_reddit1.append(di1, ignore_index = True)\n",
        "      upload_count +=1\n",
        "      # print(df_reddit1.shape,upload_count)\n",
        "      \n",
        "    \n",
        "    print(df_reddit1.columns)\n",
        "   \n",
        "    return df_reddit1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdpR6kDGuaOS"
      },
      "source": [
        "### **Reddit Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NAOKZO27DVSl",
        "outputId": "2547a545-9c34-4bf6-ecce-23e48141b249"
      },
      "source": [
        "!pip install flair\n",
        "import flair"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.9-py3-none-any.whl (319 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 319 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 28.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 28.9 MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 17.4 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 43 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.9.0+cu111)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.3.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.12.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 746 kB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.5.30)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 42.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=e97aa5daa3e537dbadd088a630d1203fea45d700572ddc570f479d2999ac17bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=4acad6e0bcb68c37d53a074ccc91726eaa43ffbb37dd82e10b9701ea88b8b3b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=04f6747c8858e0aaed5e676c6c04ffa50858cef43aefa37b394084442485970b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=a5d564db6eaa11e999ef9c58c9fd44f417cc8e669446f4961890efdc9252a1cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=7139d586cc229844fd5500ebcf1b55480d5e59ae644100e07f49b9460e3a2431\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=04f17b22b6aaee11ff4d563ea536700b5c5a12fc9bd5278c505a50f15ad6e412\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=a7f916284fc77868fa32a56fc0ae15ef90056154b404d33a9f87b774b4ee8164\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=5d461f178318140188eb0b0da1c387b0fb06db69e7d576ff2c1b13b2ca7825bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides segtok sqlitedict ftfy langdetect wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.1\n",
            "    Uninstalling importlib-metadata-4.8.1:\n",
            "      Successfully uninstalled importlib-metadata-4.8.1\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.10.0\n",
            "    Uninstalling more-itertools-8.10.0:\n",
            "      Successfully uninstalled more-itertools-8.10.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.9 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.19 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pyyaml-5.4.1 requests-2.26.0 sacremoses-0.0.46 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 transformers-4.11.3 wikipedia-api-0.5.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyszGeHUDJdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade3c4c1-13c1-4d3b-c032-8a718e82d9b9"
      },
      "source": [
        "# now sentiment analysis and bucketizing\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "import pandas as pd\n",
        "\n",
        "from textblob import TextBlob\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41c7svLqjG5t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b2f252be-4a2e-4608-f656-ef8c83c6ecd8"
      },
      "source": [
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "fmt = '%Y-%m-%d %H:00:00'\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-10 04:24:38,645 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /tmp/tmpj1triei9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265512723/265512723 [00:12<00:00, 21415751.99B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-10 04:24:51,425 copying /tmp/tmpj1triei9 to cache at /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-10 04:24:52,634 removing temp file /tmp/tmpj1triei9\n",
            "2021-10-10 04:24:54,266 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3a78d4343b74d9d9d894878eceafda9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f381faa4a904581be5d697f98226eb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ac521be901a4669b3c14f0c849fdce0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7384ebd930124883bf1af08475da486f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeKEgh4CDvlu"
      },
      "source": [
        "def get_sentiment_val_for_flair(sentiments):\n",
        "    \"\"\"\n",
        "    parse input of the format [NEGATIVE (0.9284018874168396)] and return +ve or -ve float value\n",
        "    :param sentiments:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    total_sentiment = str(sentiments)\n",
        "    neg = 'NEGATIVE' in total_sentiment\n",
        "    if neg:\n",
        "        total_sentiment = total_sentiment.replace('NEGATIVE', '')\n",
        "    else:\n",
        "        total_sentiment = total_sentiment.replace('POSITIVE', '')\n",
        "\n",
        "    total_sentiment = total_sentiment.replace('(', '').replace('[', '').replace(')', '').replace(']', '')\n",
        "\n",
        "    val = float(total_sentiment)\n",
        "    if neg:\n",
        "        return -val\n",
        "    return val"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNK3hQqQDy6i"
      },
      "source": [
        "def get_sentiment_report_reddit(df):\n",
        "    # df = pd.read_csv('/content/'+input_filename)\n",
        "    df = df[['title', 'selftext', 'publish_date']]\n",
        "    df = df.fillna('')\n",
        "\n",
        "    df['text'] = df['title'] + ' ' + df['selftext']\n",
        "    df.set_index('publish_date', inplace=True)\n",
        "    df.drop(['title', 'selftext'], axis=1, inplace=True)\n",
        "\n",
        "    columns = ['timestamp','reddit_flair','reddit_tb_polarity', 'reddit_tb_subjectivity', 'reddit_sid_pos','reddit_sid_neg', 'reddit_sid_neu', 'reddit_sid_com']\n",
        "    final_senti_df = pd.DataFrame(columns = columns)\n",
        "    final_senti_df.set_index('timestamp',inplace = True)\n",
        "\n",
        "\n",
        "    for row_i, row in df.iterrows():\n",
        "        tb_sentiment_polarity_dict = dict()\n",
        "        tb_sentiment_subjectivity_dict = dict()\n",
        "        flair_sentiment_dict = dict()\n",
        "\n",
        "        sid_pos_dict = dict()\n",
        "        sid_neg_dict = dict()\n",
        "        sid_neu_dict = dict()\n",
        "        sid_com_dict = dict()\n",
        "\n",
        "        data = row['text']\n",
        "        print(row_i)\n",
        "        print(data[0:15])\n",
        "        flair_s = flair.data.Sentence(data)\n",
        "        flair_sentiment.predict(flair_s)\n",
        "        flair_total_sentiment = flair_s.labels\n",
        "        flair_val = get_sentiment_val_for_flair(flair_total_sentiment)\n",
        "\n",
        "        flair_sentiment_dict[str(row_i)] = flair_val\n",
        "        tb_sentiment_polarity_dict[str(row_i)] = TextBlob(data).sentiment[0]\n",
        "        tb_sentiment_subjectivity_dict[str(row_i)] = TextBlob(data).sentiment[1]\n",
        "\n",
        "        ss = sid.polarity_scores(data)\n",
        "        sid_pos_dict[str(row_i)] = ss['pos']\n",
        "        sid_neg_dict[str(row_i)] = ss['neg']\n",
        "        sid_neu_dict[str(row_i)] = ss['neu']\n",
        "        sid_com_dict[str(row_i)] = ss['compound']\n",
        "\n",
        "        flair_df = pd.DataFrame.from_dict(flair_sentiment_dict, orient='index', columns=['reddit_flair'])\n",
        "        flair_df.index.name = 'timestamp'\n",
        "\n",
        "        tb_polarity_df = pd.DataFrame.from_dict(tb_sentiment_polarity_dict, orient='index',\n",
        "                                                columns=['reddit_tb_polarity'])\n",
        "        tb_polarity_df.index.name = 'timestamp'\n",
        "\n",
        "        tb_subjectivity_df = pd.DataFrame.from_dict(tb_sentiment_subjectivity_dict, orient='index',\n",
        "                                                    columns=['reddit_tb_subjectivity'])\n",
        "        tb_subjectivity_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_pos_df = pd.DataFrame.from_dict(sid_pos_dict, orient='index',\n",
        "                                            columns=['reddit_sid_pos'])\n",
        "        sid_pos_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_neg_df = pd.DataFrame.from_dict(sid_neg_dict, orient='index',\n",
        "                                            columns=['reddit_sid_neg'])\n",
        "        sid_neg_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_neu_df = pd.DataFrame.from_dict(sid_neu_dict, orient='index',\n",
        "                                            columns=['reddit_sid_neu'])\n",
        "        sid_neu_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_com_df = pd.DataFrame.from_dict(sid_com_dict, orient='index',\n",
        "                                            columns=['reddit_sid_com'])\n",
        "        sid_com_df.index.name = 'timestamp'\n",
        "\n",
        "        # di2 = dict.fromkeys(columns,'')\n",
        "        \n",
        "\n",
        "        dummy_df = pd.concat([flair_df, tb_polarity_df, tb_subjectivity_df, sid_pos_df, sid_neg_df, sid_neu_df,\n",
        "        \t\t\t\t\t\t\tsid_com_df], axis=1)\n",
        "        \n",
        "        frames=[final_senti_df,dummy_df]\n",
        "\n",
        "        final_senti_df = pd.concat(frames)\n",
        "        # display(dummy_df)\n",
        "        # final_senti_df.append(dummy_df)\n",
        "\n",
        "        # if os.path.exists(output_filename):\n",
        "        #     keep_header = False\n",
        "        # else:\n",
        "        #     keep_header = True\n",
        "\n",
        "        # final_senti_df.to_csv(output_filename, mode='a', header=keep_header)\n",
        "\n",
        "    return final_senti_df\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Th0VumD4qh"
      },
      "source": [
        "def bucketize_sentiment_report(in_df,time_begin,time_end):\n",
        "    start_date_time_obj = time_begin\n",
        "    # start_date_time_obj = time_begin - datetime.timedelta(hours=1)\n",
        "    \n",
        "    # end_date_time_obj = datetime.datetime(2019, 11, 22, 00)\n",
        "    \n",
        "    end_date_time_obj = time_end\n",
        "    hr1 = datetime.timedelta(hours=1)\n",
        "    curr_date_time_obj = start_date_time_obj\n",
        "    # in_df = pd.read_csv(\"/content/\"+input_filename)\n",
        "\n",
        "\n",
        "    out_dict = dict()\n",
        "\n",
        "    while curr_date_time_obj <= end_date_time_obj:\n",
        "        curr_timestamp = curr_date_time_obj.strftime(format=fmt)\n",
        "        # print(curr_timestamp)\n",
        "        # create data dict with all possible timestamps and dummy value of reddit_flair\n",
        "        # reddit_flair is chosen just randomly as a placeholder\n",
        "        out_dict[curr_timestamp] = 0\n",
        "        curr_date_time_obj += hr1\n",
        "\n",
        "    # print(out_dict)\n",
        "    out_df = pd.DataFrame.from_dict(out_dict, orient='index',\n",
        "                                    columns=['reddit_flair'])\n",
        "\n",
        "    # print(out_dict)\n",
        "    out_df.index.name = 'timestamp'\n",
        "    # populate more colums\n",
        "    out_df['reddit_flair_count'] = 0\n",
        "    out_df['reddit_tb_polarity'] = 0\n",
        "    out_df['reddit_tb_polarity_count'] = 0\n",
        "    out_df['reddit_tb_subjectivity'] = 0\n",
        "    out_df['reddit_tb_subjectivity_count'] = 0\n",
        "    out_df['reddit_sid_pos'] = 0\n",
        "    out_df['reddit_sid_neg'] = 0\n",
        "    out_df['reddit_sid_neu'] = 0\n",
        "    out_df['reddit_sid_com'] = 0\n",
        "    out_df['reddit_sid_count'] = 0\n",
        "\n",
        "    # print(out_df)\n",
        "    \n",
        "    in_df.reset_index(inplace = True)\n",
        "    # print(in_df)\n",
        "    for i in range(len(in_df)):\n",
        "        \n",
        "        timestamp = in_df.loc[i, 'timestamp']\n",
        "        out_key = datetime.datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
        "        # print('Ankur: start')\n",
        "        # print(out_key)\n",
        "        # timestamp is current plus few minutes or seconds, so collect all these data in the bucket of next hour\n",
        "        out_key += hr1\n",
        "        out_key = out_key.strftime(format='%Y-%m-%d %H:00:00')\n",
        "        \n",
        "        # print(out_df)\n",
        "        \n",
        "        # add up all values and count how many values we have added. In next pass we would normalize the values\n",
        "        try:\n",
        "            \n",
        "            out_df.loc[out_key, 'reddit_flair'] += in_df.loc[i, 'reddit_flair']\n",
        "            \n",
        "            out_df.loc[out_key, 'reddit_flair_count'] += 1\n",
        "            out_df.loc[out_key, 'reddit_tb_polarity'] += in_df.loc[i, 'reddit_tb_polarity']\n",
        "            out_df.loc[out_key, 'reddit_tb_polarity_count'] += 1\n",
        "            out_df.loc[out_key, 'reddit_tb_subjectivity'] += in_df.loc[i, 'reddit_tb_subjectivity']\n",
        "            \n",
        "            # print(out_df.loc[out_key, 'reddit_tb_subjectivity_count'])\n",
        "            \n",
        "            out_df.loc[out_key, 'reddit_tb_subjectivity_count'] += 1\n",
        "            \n",
        "            # print(out_df.loc[out_key, 'reddit_sid_pos'])\n",
        "            # print(in_df.loc[i, 'reddit_sid_pos'])\n",
        "            \n",
        "            out_df.loc[out_key, 'reddit_sid_pos'] += in_df.loc[i, 'reddit_sid_pos']\n",
        "            out_df.loc[out_key, 'reddit_sid_neg'] += in_df.loc[i, 'reddit_sid_neg']\n",
        "            out_df.loc[out_key, 'reddit_sid_neu'] += in_df.loc[i, 'reddit_sid_neu']\n",
        "            out_df.loc[out_key, 'reddit_sid_com'] += in_df.loc[i, 'reddit_sid_com']\n",
        "            out_df.loc[out_key, 'reddit_sid_count'] += 1\n",
        "            \n",
        "            # print(\"end\")\n",
        "        except Exception as e:\n",
        "            # print(e)\n",
        "            pass\n",
        "    print('Ankur: end')\n",
        "    # make timestamp as a column and reindex the dataframe to make loc method happy\n",
        "    out_df['timestamp'] = out_df.index\n",
        "    out_df.index = range(len(out_df))\n",
        "\n",
        "    for i in range(len(out_df)):\n",
        "        #print(out_df.loc[i, 'timestamp'])\n",
        "        # normalize the values\n",
        "        if out_df.loc[i, 'reddit_flair_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_flair'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_flair'] /= out_df.loc[i, 'reddit_flair_count']\n",
        "\n",
        "        if out_df.loc[i, 'reddit_tb_polarity_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_tb_polarity'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_tb_polarity'] /= out_df.loc[i, 'reddit_tb_polarity_count']\n",
        "\n",
        "        if out_df.loc[i, 'reddit_tb_subjectivity_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_tb_subjectivity'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_tb_subjectivity'] /= out_df.loc[i, 'reddit_tb_subjectivity_count']\n",
        "\n",
        "        if out_df.loc[i, 'reddit_sid_count'] == 0:\n",
        "            out_df.loc[i, 'reddit_sid_pos'] = 0\n",
        "            out_df.loc[i, 'reddit_sid_neg'] = 0\n",
        "            out_df.loc[i, 'reddit_sid_neu'] = 0\n",
        "            out_df.loc[i, 'reddit_sid_com'] = 0\n",
        "        else:\n",
        "            out_df.loc[i, 'reddit_sid_pos'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "            out_df.loc[i, 'reddit_sid_neg'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "            out_df.loc[i, 'reddit_sid_neu'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "            out_df.loc[i, 'reddit_sid_com'] /= out_df.loc[i, 'reddit_sid_count']\n",
        "\n",
        "        # if os.path.exists(output_filename):\n",
        "        #     keep_header = False\n",
        "        # else:\n",
        "        #     keep_header = True\n",
        "\n",
        "    out_df.drop(['reddit_flair_count', 'reddit_tb_polarity_count', 'reddit_tb_subjectivity_count','reddit_sid_count'], axis=1,\n",
        "                inplace=True)\n",
        "    # change back index to timestamp to save the data in csv\n",
        "    out_df.set_index('timestamp', inplace=True)\n",
        "    return out_df\n",
        "    # out_df.to_csv(output_filename)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b75RW9sh4Mq"
      },
      "source": [
        "### **Web Scrapping Segment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRdDYTjSjNmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4f9f21-3120-4115-9260-ca23480246c4"
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (5.4.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.26.0)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.3.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=aa2f77098be2b21986f670e426d942749db4f4bb941752176cbbd56f11c772d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=b3089f83713e60df1b9febc9ba7fc2f12e0818678eae9906ec1159a9738b94d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=3a888e008c61100092804145630eb1542b04b65c4457ba089b54c5b6964f1b8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=dca370f37d7e585eed40be65edc68db9f1d0f4f4806ccb53637267b17fc9cc69\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZHo7f_Qh-wR"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pytz"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqMw9gQMjLBX"
      },
      "source": [
        "URL = \"https://www.google.com/search?q=cryptocurrency+bitcoin+after%3A{after_date}+before%3A{before_date}&hl=en&biw=1536&bih=731&tbs=sbd%3A1&tbm=nws&sxsrf=AOaemvJvsALAxFs9-Zs_bHUW8HthpSttsw%3A1631421050994&ei=eoI9YZicPOCUr7wP9MGg0A8&oq=cryptocurrency+after%3A{after_date}+before%3A{before_date}&gs_l=psy-ab.3...5565.5565.0.6063.1.1.0.0.0.0.156.156.0j1.1.0....0...1c.1.64.psy-ab..0.0.0....0.e0I-wakyIOk\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
        "    'Content-Type': 'text/html',\n",
        "}\n",
        "\n",
        "max_count = 10  # max 10 news articles per day\n",
        "fmt = '%m/%d/%Y'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nk2I2j2jkVu"
      },
      "source": [
        "def run_google_news_scrapper(**params):\n",
        "\n",
        "    news_cols = ['date','news_1_text',\n",
        "             'news_2_text',  'news_3_text',\n",
        "             'news_4_text', 'news_5_text',\n",
        "            'news_6_text','news_7_text',\n",
        "             'news_8_text', 'news_9_text']\n",
        "\n",
        "    news_data_df = pd.DataFrame(index=[0], columns=news_cols)\n",
        "    \n",
        "    for key, value in params.items():\n",
        "        if key == 'min_date':\n",
        "            min_date = value\n",
        "        if key == 'output_file':\n",
        "            output_file_name = value\n",
        "\n",
        "    after_date = min_date + datetime.timedelta(days = -1)\n",
        "    before_date = min_date\n",
        "    x = str(after_date)\n",
        "    x = x.split(' ')[0]\n",
        "    y = str(before_date)\n",
        "    y = y.split(' ')[0]\n",
        "    url = URL.format(after_date = x, before_date = y)\n",
        "    print(\"this is url:\",url)\n",
        "    print(type(min_date))\n",
        "    news_data_dict = dict()\n",
        "    print(min_date)\n",
        "    news_data_dict['date'] = min_date\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    #     print(soup)\n",
        "    #news_data_dict['status_code'] = response.status_code\n",
        "    #     columns.append('status_code')\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        print(\"* fail *** \")\n",
        "        return\n",
        "    # print(response.url)\n",
        "    #news_data_dict['url'] = response.url\n",
        "\n",
        "    count = 1\n",
        "\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        # time.sleep(1)\n",
        "        link_str = str(link.get('href'))\n",
        "\n",
        "        # print(link_str)\n",
        "        # print(link_str.find(\"url?q\"),\"  \",link_str)\n",
        "        \n",
        "        if (link_str.find(\"google.com\") != -1): continue\n",
        "        if (link_str.find(\"coindesk.com\") != -1): continue\n",
        "        if (link_str.find(\"bloomberg.com\") != -1): continue\n",
        "        no = link_str.find(\"url?q\")\n",
        "        if no != -1:\n",
        "            ind = link_str.find(\"&sa=U&ved\")\n",
        "            links.append(link_str[7:ind])\n",
        "\n",
        "    links = set(links)\n",
        "    links = list(links)\n",
        "    for link_str in links:\n",
        "        # link_str = str(link.get('url'))\n",
        "        # print(link_str)\n",
        "        try:\n",
        "           \n",
        "                article = Article(link_str)\n",
        "                article.download()\n",
        "                \n",
        "                article.parse()\n",
        "                #print(link_str)\n",
        "                #                 print(article.authors)\n",
        "                #                 print(article.publish_date)\n",
        "                #                 print(article.text)\n",
        "                news_count = 'news_' + str(count)\n",
        "\n",
        "                #news_data_dict[news_count + '_url'] = link_str\n",
        "                news_data_dict[news_count + '_text'] = article.text\n",
        "                \n",
        "                count += 1\n",
        "                if count >= max_count:\n",
        "                    break\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "       \n",
        "    news_data_df = news_data_df.append(news_data_dict, ignore_index=True)\n",
        "    \n",
        "\n",
        "\n",
        "    # if os.path.exists(output_file_name):\n",
        "    #     keep_header = False\n",
        "    # else:\n",
        "    #     keep_header = True\n",
        "    # news_data_df.to_csv(output_file_name, mode='a', header=keep_header)\n",
        "\n",
        "\n",
        "#     news_data_df.to_csv(output_file_name)\n",
        "\n",
        "\n",
        "\n",
        "    return news_data_dict\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u42K_e4Uj1xC"
      },
      "source": [
        "def google_news_scrapper(start_date_inp, end_date_inp):\n",
        "    step_obj = datetime.timedelta(days=1)\n",
        "    # start_date_time_obj = datetime.datetime(2019, 11, 20)\n",
        "    # end_date_time_obj = datetime.datetime(2021, 9, 11)\n",
        "\n",
        "    start_date_time_obj = start_date_inp\n",
        "    end_date_time_obj = end_date_inp\n",
        "    #start_date_time_obj = start_date_time_obj.replace(tzinfos=pytz.UTC)\n",
        "    #end_date_time_obj = end_date_time_obj.replace(tzinfos=pytz.UTC)\n",
        "\n",
        "\n",
        "    news_cols = ['date','news_1_text',\n",
        "             'news_2_text',  'news_3_text',\n",
        "             'news_4_text', 'news_5_text',\n",
        "            'news_6_text','news_7_text',\n",
        "             'news_8_text', 'news_9_text']\n",
        "\n",
        "    news_data_df = pd.DataFrame(index=[0], columns=news_cols)\n",
        "    \n",
        "    while start_date_time_obj <= end_date_time_obj:\n",
        "        start_date = start_date_time_obj\n",
        "        print(start_date)\n",
        "        dummy_df_2 = run_google_news_scrapper(min_date=start_date, max_date=start_date)\n",
        "        news_data_df = news_data_df.append(dummy_df_2, ignore_index=True)\n",
        "        time.sleep(np.random.randint(2, 5))\n",
        "        start_date_time_obj += step_obj\n",
        "    # news_data_df.to_csv(output_file_name)\n",
        "\n",
        "    return news_data_df\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btVyRUuMkPHY"
      },
      "source": [
        "def sort_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    df = pd.read_csv(input_file_name)\n",
        "    df = df.set_index('date', drop=True)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.sort_index().drop_duplicates(keep='first')\n",
        "    df.to_csv(cleaned_output_file_name)\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st5y4EzUkSYV"
      },
      "source": [
        "def clean_news_report(input_df,  save_index=False):\n",
        "    master_df = input_df\n",
        "    # get only given columns\n",
        "    master_df = master_df[\n",
        "        ['date', 'news_1_text', 'news_2_text', 'news_3_text', 'news_4_text', 'news_5_text', 'news_6_text',\n",
        "         'news_7_text', 'news_8_text', 'news_9_text']]\n",
        "    master_df = master_df.set_index('date', drop=True)\n",
        "    #master_df.index = pd.to_datetime(master_df.index, format=fmt)\n",
        "    # soft and drop duplicates\n",
        "    master_df = master_df.sort_index().drop_duplicates(keep='first')\n",
        "    #idx = np.unique(master_df.index, return_index=True)[1]\n",
        "    #master_df = master_df.iloc[idx]\n",
        "    #     master_df.to_csv(cleaned_output_file_name)\n",
        "\n",
        "    # master_df.to_csv(cleaned_output_file_name)\n",
        "    return master_df\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(master_df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPvIH9mqz08"
      },
      "source": [
        "### **Web Scrapping - newz - sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJv5focdkVP5"
      },
      "source": [
        "import pandas as pd\n",
        "import flair\n",
        "from textblob import TextBlob\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "fmt = '%Y-%m-%d'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAGARQXxq4OA"
      },
      "source": [
        "def get_sentiment_val_for_flair(sentiments):\n",
        "    \"\"\"\n",
        "    parse input of the format [NEGATIVE (0.9284018874168396)] and return +ve or -ve float value\n",
        "    :param sentiments:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    total_sentiment = str(sentiments)\n",
        "    print(total_sentiment)\n",
        "    neg = 'NEGATIVE' in total_sentiment\n",
        "    if neg:\n",
        "        total_sentiment = total_sentiment.replace('NEGATIVE', '')\n",
        "    else:\n",
        "        total_sentiment = total_sentiment.replace('POSITIVE', '')\n",
        "\n",
        "    total_sentiment = total_sentiment.replace('(', '').replace('[', '').replace(')', '').replace(']', '')\n",
        "\n",
        "    print(total_sentiment)\n",
        "    if total_sentiment == '':\n",
        "      val=0\n",
        "    else:\n",
        "      val = float(total_sentiment)\n",
        "    if neg:\n",
        "        return -val\n",
        "    return val"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOtychjdq9BK"
      },
      "source": [
        "def add_to_dict(final_dict, input_dict):\n",
        "    \"\"\"\n",
        "    add matching key values and store final result in final_dict\n",
        "    :param final_dict:\n",
        "    :param input_dict:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    for item in final_dict:\n",
        "        input_dict_val = input_dict.get(item, 0)\n",
        "        final_dict[item] += input_dict_val"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39cVkV8q_kO"
      },
      "source": [
        "def devide_dict_by_scaler(in_dict, val):\n",
        "    \"\"\"\n",
        "    devide each value of dict by scaler\n",
        "    :param in_dict:\n",
        "    :param val:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    for item in in_dict:\n",
        "        in_dict[item] /= val\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYbovMAQrBq-"
      },
      "source": [
        "def get_sentiment_report(data_df, start_date=None, simulate=False):\n",
        "    \"\"\"\n",
        "\n",
        "    :param data_df: input data is panda dataframe, with index as date of the format fmt\n",
        "    :return: another dataframe with same index as input dataframe and new columns as sentiment values\n",
        "    \"\"\"\n",
        "    # data_df = pd.read_csv(input_filename, index_col=0)\n",
        "    data_df.reset_index(inplace=True)\n",
        "    # display(data_df)\n",
        "    data_df = data_df.dropna(subset=['date'])\n",
        "    data_df.set_index('date',inplace=True)\n",
        "    # print(data_df.index)\n",
        "\n",
        "    col = data_df.columns\n",
        "\n",
        "    # sid = SentimentIntensityAnalyzer()\n",
        "    if simulate:\n",
        "        flair_sentiment = None\n",
        "        sid = None\n",
        "    else:\n",
        "        flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "        sid = SentimentIntensityAnalyzer()\n",
        "    temp_c = 0\n",
        "    for row_i, row in data_df.iterrows():\n",
        "        temp_c += 1\n",
        "        # print(row_i)\n",
        "\n",
        "        if start_date is not None:\n",
        "            start_date_time_obj = datetime.datetime.strptime(start_date, fmt)\n",
        "            current_date_time_obj = datetime.datetime.strptime(str(row_i), fmt)\n",
        "\n",
        "            if current_date_time_obj < start_date_time_obj:\n",
        "                print('Skipping record of date ', str(current_date_time_obj), ' But looking for ',\n",
        "                      str(start_date_time_obj))\n",
        "                continue\n",
        "\n",
        "        total_sentiment_data_count = 0\n",
        "        tb_sentiment_polarity_dict = dict()\n",
        "        tb_sentiment_subjectivity_dict = dict()\n",
        "        flair_sentiment_dict = dict()\n",
        "\n",
        "        sid_pos_dict = dict()\n",
        "        sid_neg_dict = dict()\n",
        "        sid_neu_dict = dict()\n",
        "        sid_com_dict = dict()\n",
        "\n",
        "        flair_sentiment_total = 0\n",
        "\n",
        "        tb_polarity_total = 0\n",
        "        tb_subjectivity_total = 0\n",
        "\n",
        "        sid_pos_total = 0\n",
        "        sid_neg_total = 0\n",
        "        sid_neu_total = 0\n",
        "        sid_com_total = 0\n",
        "\n",
        "        # sid_polarity_total = {'neg': 0., 'neu': 0., 'pos': 0., 'compound': 0.}\n",
        "\n",
        "        for col_i in range(len(col)):\n",
        "            data = (str(row[col_i]))\n",
        "            # print('\\t', col_i)\n",
        "            if data == 'NaN':\n",
        "                continue\n",
        "\n",
        "            if simulate:\n",
        "                flair_sentiment_total = 5\n",
        "                tb_polarity_total = 6\n",
        "                tb_subjectivity_total = 7\n",
        "                total_sentiment_data_count = 9\n",
        "            else:\n",
        "                tb_polarity_total += TextBlob(data).sentiment[0]\n",
        "                tb_subjectivity_total += TextBlob(data).sentiment[1]\n",
        "\n",
        "                flair_s = flair.data.Sentence(data)\n",
        "                flair_sentiment.predict(flair_s)\n",
        "                flair_total_sentiment = flair_s.labels\n",
        "                flair_val = get_sentiment_val_for_flair(flair_total_sentiment)\n",
        "                flair_sentiment_total += flair_val\n",
        "\n",
        "                ss = sid.polarity_scores(data)\n",
        "                sid_pos_total += ss['pos']\n",
        "                sid_neg_total += ss['neg']\n",
        "                sid_neu_total += ss['neu']\n",
        "                sid_com_total += ss['compound']\n",
        "\n",
        "                total_sentiment_data_count += 1\n",
        "\n",
        "        print(str(row_i), ' ', temp_c)\n",
        "        flair_sentiment_dict[str(row_i)] = flair_sentiment_total / total_sentiment_data_count\n",
        "        tb_sentiment_polarity_dict[str(row_i)] = tb_polarity_total / total_sentiment_data_count\n",
        "        tb_sentiment_subjectivity_dict[str(row_i)] = tb_subjectivity_total / total_sentiment_data_count\n",
        "        print(flair_sentiment_dict[str(row_i)], tb_sentiment_polarity_dict[str(row_i)],\n",
        "              tb_sentiment_subjectivity_dict[str(row_i)])\n",
        "\n",
        "        flair_df = pd.DataFrame.from_dict(flair_sentiment_dict, orient='index', columns=['gnews_flair'])\n",
        "        flair_df.index.name = 'date'\n",
        "\n",
        "        tb_polarity_df = pd.DataFrame.from_dict(tb_sentiment_polarity_dict, orient='index',\n",
        "                                                columns=['gnews_tb_polarity'])\n",
        "        tb_polarity_df.index.name = 'date'\n",
        "\n",
        "        tb_subjectivity_df = pd.DataFrame.from_dict(tb_sentiment_subjectivity_dict, orient='index',\n",
        "                                                    columns=['gnews_tb_subjectivity'])\n",
        "        tb_subjectivity_df.index.name = 'date'\n",
        "\n",
        "        sid_pos_dict[str(row_i)] = sid_pos_total / total_sentiment_data_count\n",
        "        sid_neg_dict[str(row_i)] = sid_neg_total / total_sentiment_data_count\n",
        "        sid_neu_dict[str(row_i)] = sid_neu_total / total_sentiment_data_count\n",
        "        sid_com_dict[str(row_i)] = sid_com_total / total_sentiment_data_count\n",
        "\n",
        "        sid_pos_df = pd.DataFrame.from_dict(sid_pos_dict, orient='index',\n",
        "                                            columns=['gnews_sid_pos'])\n",
        "        sid_pos_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_neg_df = pd.DataFrame.from_dict(sid_neg_dict, orient='index',\n",
        "                                            columns=['gnews_sid_neg'])\n",
        "        sid_neg_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_neu_df = pd.DataFrame.from_dict(sid_neu_dict, orient='index',\n",
        "                                            columns=['gnews_sid_neu'])\n",
        "        sid_neu_df.index.name = 'timestamp'\n",
        "\n",
        "        sid_com_df = pd.DataFrame.from_dict(sid_com_dict, orient='index',\n",
        "                                            columns=['gnews_sid_com'])\n",
        "        sid_com_df.index.name = 'timestamp'\n",
        "\n",
        "        final_senti_df = pd.concat([flair_df, tb_polarity_df, tb_subjectivity_df, sid_pos_df, sid_neg_df,\n",
        "                            sid_neu_df, sid_com_df], axis=1)\n",
        "\n",
        "        # if os.path.exists(output_filename):\n",
        "        #     keep_header = False\n",
        "        # else:\n",
        "        #     keep_header = True\n",
        "\n",
        "        # final_senti_df.to_csv(output_filename, mode='a', header=keep_header)\n",
        "\n",
        "    return final_senti_df\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDDNToJ6rHPp"
      },
      "source": [
        "def clean_sentiment_report(master_df):\n",
        "    # drop duplicates and sort\n",
        "    # master_df = pd.read_csv(input_filename, index_col=0)\n",
        "    master_df.index = pd.to_datetime(master_df.index)\n",
        "    idx = np.unique(master_df.index, return_index=True)[1]\n",
        "    master_df = master_df.iloc[idx]\n",
        "    return master_df"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXLL7LhU6FIE"
      },
      "source": [
        "### **Crypto Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zKMYbTi8MFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d1c078-85e3-4d11-a9db-6b42905cf5c9"
      },
      "source": [
        "!pip install python-binance"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-binance\n",
            "  Downloading python_binance-1.0.15-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 20 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 30 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 40 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting dateparser\n",
            "  Downloading dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 122 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 163 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 174 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 184 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 204 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 215 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 245 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 256 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 266 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 276 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 286 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 288 kB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-binance) (1.15.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting ujson\n",
            "  Downloading ujson-4.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214 kB)\n",
            "\u001b[K     |████████████████████████████████| 214 kB 71.2 MB/s \n",
            "\u001b[?25hCollecting websockets==9.1\n",
            "  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from python-binance) (2.26.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->python-binance) (3.7.4.3)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->python-binance) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->python-binance) (21.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->python-binance) (2.10)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (2018.9)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.7/dist-packages (from dateparser->python-binance) (2019.12.20)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->python-binance) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->python-binance) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->python-binance) (1.24.3)\n",
            "Installing collected packages: multidict, yarl, async-timeout, websockets, ujson, dateparser, aiohttp, python-binance\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 dateparser-1.1.0 multidict-5.2.0 python-binance-1.0.15 ujson-4.2.0 websockets-9.1 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSPzw7W86cUb"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from binance.client import Client\n",
        "\n",
        "import pytz\n",
        "import numpy as np\n",
        "import datetime\n",
        "from datetime import timedelta"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQpAC8C49IWf"
      },
      "source": [
        "epoch = 0\n",
        "fmt = \"%Y-%m-%d %H:%M:%S\"  # e.g. 2019-11-16 23:16:15\n",
        "org_columns = ['open',\n",
        "               'high', 'low', 'close', 'volume', 'close_time', 'quote_av',\n",
        "               'trades', 'tb_base_av', 'tb_quote_av', 'ignore']\n",
        "\n",
        "columns_of_interest = ['open', 'high', 'low', 'close', 'volume']\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TSjZTUb9OiT"
      },
      "source": [
        "def init_mod():\n",
        "    # refer: https://www.binance.com/en/support/articles/360002502072 for API keys\n",
        "    binance_api_key = \"786lfq19MTn1G7cB4mozqbjuvd3u2Fl9I0J0spsnflf3SlP3yZIwEdMBANdHN8lV\"\n",
        "    binance_api_secret = \"uuAmPtxoxOtqE1wi4KCEzEZZHnATo8pwbIsa8LExM1Y7vGBuaC0kapL7ut4YRfwF\"\n",
        "    binance_client = Client(api_key=binance_api_key, api_secret=binance_api_secret)\n",
        "    global epoch\n",
        "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
        "    return binance_client"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zh0Np4p9RRu"
      },
      "source": [
        "def convert_time_to_utc(pst_time):\n",
        "    utc = pytz.utc\n",
        "    pst = pytz.timezone('America/Los_Angeles')\n",
        "    datetime1 = datetime.datetime.strptime(pst_time, fmt)\n",
        "    pst_time = pst.localize(datetime1)\n",
        "    return pst_time.astimezone(utc).strftime(fmt)\n",
        "\n",
        "\n",
        "def convert_time_to_pst(utc_time):\n",
        "    datetime_obj = datetime.datetime.strptime(utc_time, fmt)\n",
        "    return datetime_obj.replace(tzinfo=time.timezone('UTC')).strftime(fmt)\n",
        "\n",
        "\n",
        "def to_unixmillis(from_date):\n",
        "    from_date_obj = datetime.datetime.strptime(from_date, fmt)\n",
        "    past = datetime.datetime(1970, 1, 1, tzinfo=from_date_obj.tzinfo)\n",
        "    return int((from_date_obj - past).total_seconds() * 1000.0)\n",
        "\n",
        "\n",
        "def to_datetime(ms):\n",
        "    return datetime.datetime.fromtimestamp(int(float(ms) / 1000.0))\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCiOwpSe9WoS"
      },
      "source": [
        "def download_data_from_binance(symbol, from_date, to_date,  step=0, pause=-1, simulate=False):\n",
        "    \"\"\"\n",
        "\n",
        "    :param symbol:\n",
        "    :param from_date:\n",
        "    :param to_date:\n",
        "    :param output_filename:\n",
        "    :param step: step in number of days. Download data in batches of days given by 'step'\n",
        "    :param pause: pause seconds before downloading next batch.\n",
        "        if pause == -1 --> random sleep(2,5)\n",
        "        if pause == 0 --> no sleep\n",
        "        if pause == num--> sleep for num of seconds\n",
        "    :param simulate:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    binance_client = init_mod()\n",
        "    from_date_obj = datetime.datetime.strptime(from_date, fmt)\n",
        "    # from_date_obj = from_date\n",
        "    step_date_obj = from_date_obj + timedelta(days=step)\n",
        "    step_date = step_date_obj.strftime(fmt)\n",
        "\n",
        "    from_millis = to_unixmillis(from_date)\n",
        "    to_millis = to_unixmillis(to_date)\n",
        "    step_millis = to_unixmillis(step_date)\n",
        "\n",
        "    count = 0\n",
        "    while True:\n",
        "        from_millis_str = str(from_millis)\n",
        "        step_millis_str = str(step_millis)\n",
        "        print('Step %d:Downloading data from %s to %s' % (count,\n",
        "                                                          str(to_datetime(from_millis_str)),\n",
        "                                                          str(to_datetime(step_millis_str))\n",
        "                                                          ))\n",
        "        if not simulate:\n",
        "            # download data\n",
        "\n",
        "            klines = binance_client.get_historical_klines(symbol, Client.KLINE_INTERVAL_1HOUR,\n",
        "                                                          from_millis_str, end_str=step_millis_str)\n",
        "            klines_len = len(klines)\n",
        "            if klines_len == 0:\n",
        "                print('\\t Failed to download from %s to %s. Got %d' % (str(to_datetime(from_millis_str)),\n",
        "                                                                       str(to_datetime(step_millis_str)), klines_len\n",
        "                                                                       ))\n",
        "                time.sleep(5)\n",
        "\n",
        "            print('\\t Downloaded data of len %d from %s to %s' % (klines_len,\n",
        "                                                                  str(to_datetime(from_millis_str)),\n",
        "                                                                  str(to_datetime(step_millis_str))\n",
        "                                                                  ))\n",
        "            new_columns = [item + '_' + symbol for item in org_columns]\n",
        "            new_columns.insert(0, 'timestamp')\n",
        "\n",
        "            data_df = pd.DataFrame(klines,\n",
        "                                   columns=new_columns)\n",
        "            data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], unit='ms')\n",
        "            data_df.set_index('timestamp', inplace=True)\n",
        "            # data_df.to_csv(output_filename)\n",
        "            # display(data_df)\n",
        "            return  data_df \n",
        "\n",
        "        # move to next step of batches\n",
        "        from_millis = step_millis\n",
        "        step_date_obj = step_date_obj + timedelta(days=step)\n",
        "        step_date = step_date_obj.strftime(fmt)\n",
        "        step_millis = to_unixmillis(step_date)\n",
        "        count = count + 1\n",
        "        if pause == -1:\n",
        "            pause = np.random.randint(2, 5)\n",
        "        time.sleep(pause)\n",
        "        if step_millis >= to_millis:\n",
        "            break\n",
        "           \n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-JB2Jkd9dpK"
      },
      "source": [
        "def concat_binance_data(ltc,eth,btc):\n",
        "    df_list = []\n",
        "    # for num, symbol in enumerate(symbol_list):\n",
        "    #     filename = str('%s-binance-data.csv' % (symbol))\n",
        "    #     df = pd.read_csv(filename, index_col=0)\n",
        "    #     df_list.append(df)\n",
        "    df_list.append(ltc)\n",
        "    df_list.append(eth)\n",
        "    df_list.append(btc)\n",
        "\n",
        "    result = pd.concat(df_list, axis=1, sort=True)\n",
        "    result.index = pd.to_datetime(df.index)\n",
        "    result = result.sort_index().drop_duplicates(keep='first')\n",
        "    idx = np.unique(result.index, return_index=True)[1]\n",
        "    result = result.iloc[idx]\n",
        "\n",
        "    new_columns = [item + '_' + 'BTCUSDT' for item in columns_of_interest]\n",
        "    # new_columns.insert(0, 'timestamp')\n",
        "\n",
        "    for num, symbol in enumerate(symbol_list):\n",
        "        if symbol == 'BTCUSDT':\n",
        "            continue\n",
        "        new_columns.append('close_' + symbol)\n",
        "        new_columns.append('volume_' + symbol)\n",
        "\n",
        "    result = result[new_columns]\n",
        "    # result.to_csv(output_filename)\n",
        "    return result"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAMVpLS9jvG"
      },
      "source": [
        "def remove_dup_by_index(output_filename):\n",
        "    result = pd.read_csv(output_filename, index_col=0)\n",
        "    result.index = pd.to_datetime(result.index)\n",
        "    result = result.sort_index()        #.drop_duplicates(keep='first')\n",
        "    idx = np.unique(result.index, return_index=True)[1]\n",
        "    result = result.iloc[idx]\n",
        "    result.to_csv(output_filename)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOUtzDgz9n9_"
      },
      "source": [
        "def append_binance_data(master_output_filename, concat_output_filename):\n",
        "    master_df = pd.read_csv(master_output_filename)\n",
        "    new_df = pd.read_csv(concat_output_filename)\n",
        "    master_df = master_df.append(new_df)\n",
        "    master_df.set_index('timestamp', inplace=True)\n",
        "    master_df.index = pd.to_datetime(master_df.index)\n",
        "    master_df = master_df.sort_index().drop_duplicates(keep='first')\n",
        "    master_df.to_csv(master_output_filename)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjhK8MlHPFSi"
      },
      "source": [
        "### **Merge dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB4No3itPJn-"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9igRnNONPuS5"
      },
      "source": [
        "def list_diff(list1, list2):\n",
        "    \"\"\"\"\n",
        "    returns list1 - list2 and list2 - list1\n",
        "    to find difference between 2 lists.\n",
        "    \"\"\"\n",
        "    l1_l2 = []\n",
        "    l2_l1 = []\n",
        "    for ele in list1:\n",
        "        if not ele in list2:\n",
        "            l1_l2.append(ele)\n",
        "\n",
        "    for ele in list2:\n",
        "        if not ele in list1:\n",
        "            l2_l1.append(ele)\n",
        "\n",
        "    return l1_l2, l2_l1\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be8NS3rtPxJ5"
      },
      "source": [
        "def merge_crypto_gnews_sentiment(crypto_data,gnews_data):\n",
        "    \"\"\"\n",
        "\n",
        "    News data is per day, and crypto data is per hour.\n",
        "    So news data is replicated 24 times for each hour w.r.t. each day so that it can be concatinated with crypto data\n",
        "\n",
        "    :param crypto_data_filename:\n",
        "    :param google_news_data_filename:\n",
        "    :param output_data_filename:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # crypto_df = pd.read_csv(crypto_data_filename, index_col=0)\n",
        "    crypto_df = crypto_data\n",
        "    crypto_df.index = pd.to_datetime(crypto_df.index)\n",
        "    # news_df = pd.read_csv(google_news_data_filename, index_col=0)\n",
        "    news_df = gnews_data\n",
        "    # print(news_df.index)\n",
        "    ilist = [str(d) for d in news_df.index]\n",
        "    \n",
        "    # print(type(ilist[0]))\n",
        "    \n",
        "    hr1 = timedelta(hours=1)\n",
        "    news_col = list(news_df.columns)\n",
        "    news_df_comb = pd.DataFrame(columns=news_col)\n",
        "\n",
        "    print('enu',enumerate(ilist))\n",
        "    for ilist_index, ilist_item in enumerate(ilist):\n",
        "        \n",
        "        dt = datetime.datetime.strptime(ilist_item, '%Y-%m-%d %H:%M:%S')\n",
        "        # dt = datetime.datetime.strptime(ilist_item, '%d-%m-%Y')\n",
        "        for hr_ in range(0, 24):\n",
        "            row_id_new = dt.strftime('%Y-%m-%d %H:00:00')\n",
        "            row_id_value = news_df.loc[ilist_item]\n",
        "            dt += hr1\n",
        "            news_df_comb.loc[row_id_new] = row_id_value\n",
        "    news_df_comb.index.name = 'timestamp'\n",
        "    # news_df_comb.to_csv(google_news_data_filename[0:-4] + '_with_timestamp.csv')\n",
        "    result = pd.concat([crypto_df, news_df_comb], axis=1)\n",
        "    # result.to_csv(output_data_filename)\n",
        "\n",
        "    return news_df_comb,result"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAi9mL6IPzxj"
      },
      "source": [
        "def merge_crypto_gnews_reddit_sentiment(crypto_gnews, reddit_data):\n",
        "    # display(crypto_gnews)\n",
        "    # display( reddit_data)\n",
        "    \n",
        "    # print(crypto_gnews.index)\n",
        "    # print(reddit_data.index)\n",
        "    # crypto_gnews_df = pd.read_csv(crypto_gnews_filename, index_col=0)\n",
        "    crypto_gnews_df = crypto_gnews\n",
        "\n",
        "    # reddit_df = pd.read_csv(reddit_data_filename, index_col=0)\n",
        "    reddit_df = reddit_data\n",
        "    reddit_df.index = pd.to_datetime(reddit_df.index)\n",
        "    \n",
        "    # print(crypto_gnews.index)\n",
        "    # print(reddit_data.index)\n",
        "    \n",
        "    result = pd.concat([crypto_gnews_df, reddit_df], axis=1)\n",
        "   \n",
        "    # result.to_csv(crypto_gnews_reddit_filename)\n",
        "    return result"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCNvdzmGvHC3"
      },
      "source": [
        "### **run function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXwziznB1P8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ac2b06-e4e4-4ce1-9cc7-458beecc761a"
      },
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import time\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as plticker\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import keras\n",
        "\n",
        "from numpy import newaxis\n",
        "\n",
        "import time\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials,db,firestore\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VO-C-Uchb0x"
      },
      "source": [
        "def create_dataset(dataset, pred_col, look_back=1):\n",
        "  dataX, dataY = [], []\n",
        "  \n",
        "  for i in range(len(dataset)-look_back):\n",
        "    a = dataset[i:(i+look_back), :]\n",
        "    dataX.append(a)\n",
        "    dataY.append(dataset[i + look_back, pred_col])\n",
        "  \n",
        "  return np.array(dataX), np.array(dataY)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxG431pAvNVj"
      },
      "source": [
        "\n",
        "\n",
        "def run_script():\n",
        "\n",
        "  date_today = datetime.date.today()\n",
        "  date_yday = date_today - datetime.timedelta(days = 1)\n",
        "  time_now = datetime.datetime.now()\n",
        "  this_hour = time_now.replace(minute=0,second=0,microsecond=0)\n",
        "  \n",
        "  last_hour = this_hour - datetime.timedelta(hours =1)\n",
        "  print('Time Now ',time_now)\n",
        "  print('Last Hour',last_hour)\n",
        "  print('date_yday ',date_yday)\n",
        "  print('this hour ',this_hour)\n",
        "  \n",
        "  sub_reddit = 'bitcoin'\n",
        "  key_word = 'bitcoin'\n",
        "  after = str(int(last_hour.timestamp()))\n",
        "  \n",
        "  global sub_stats\n",
        "  sub_count = 0\n",
        "  sub_stats = {}\n",
        "  \n",
        "  reddit_data = get_pushshift_data(key_word, after, sub_reddit)\n",
        "  print('reddit data ',reddit_data)\n",
        "  max_count = 100\n",
        "  count = 0\n",
        "  \n",
        "  while len(reddit_data) > 0 and count < max_count:\n",
        "    print('count ', count)\n",
        "    for submission in reddit_data:\n",
        "      collect_sub_data(submission)\n",
        "      sub_count += 1\n",
        "    \n",
        "    print(len(reddit_data))\n",
        "    print(str(datetime.datetime.fromtimestamp(reddit_data[-1]['created_utc'])))\n",
        "    after = reddit_data[-1]['created_utc']\n",
        "    reddit_data = get_pushshift_data(key_word, after, sub_reddit)\n",
        "    count = count + 1\n",
        "    \n",
        "    \n",
        "  df_reddit=write_subs_to_file()\n",
        "  df_reddit_sentiment = get_sentiment_report_reddit(df_reddit)\n",
        "  \n",
        "  df_reddit_sentiment_bucketized = bucketize_sentiment_report(df_reddit_sentiment,last_hour,time_now)\n",
        "  print(pd.DataFrame())\n",
        "  \n",
        "  \n",
        "  df_news = google_news_scrapper(date_yday, date_today)\n",
        "  \n",
        "  df_news_clean = clean_news_report(df_news)\n",
        "  \n",
        "  df_news_clean_senti = get_sentiment_report(df_news_clean, simulate=False)\n",
        "  df_news_clean_senti_final = clean_sentiment_report(df_news_clean_senti)\n",
        "  \n",
        "  global symbol_list\n",
        "  symbol_list = ['LTCUSDT', 'ETHUSDT', 'BTCUSDT']\n",
        "  \n",
        "  from_date = last_hour.strftime('%Y-%m-%d %H:00:00')\n",
        "  \n",
        "  to_date = time_now.strftime('%Y-%m-%d %H:00:00')\n",
        "  \n",
        "  \n",
        "  for num, symbol in enumerate(symbol_list):\n",
        "    output_filename = '%s-binance-data.csv' % (symbol)\n",
        "    print('-' * 60)\n",
        "    print('Downloading data from %s to %s for %s' % (last_hour, to_date, symbol))\n",
        "    print('-' * 60)\n",
        "    \n",
        "    global df\n",
        "    df = download_data_from_binance(symbol, from_date, to_date,  step=1, pause=-1, simulate=False)\n",
        "    if symbol == 'LTCUSDT':\n",
        "      df_LTC = df\n",
        "    if symbol == 'ETHUSDT':\n",
        "      df_ETH = df\n",
        "    if symbol == 'BTCUSDT':\n",
        "      df_BTC = df\n",
        "      \n",
        "  df_concat_crypto = concat_binance_data(df_LTC,df_ETH,df_BTC)\n",
        "  \n",
        "  df_check,df_crypto_gnews = merge_crypto_gnews_sentiment(df_concat_crypto, df_news_clean_senti_final)\n",
        "  \n",
        "  final_df = merge_crypto_gnews_reddit_sentiment(df_crypto_gnews, df_reddit_sentiment_bucketized)\n",
        "  final_df = final_df.dropna()\n",
        "  \n",
        "  df_csv = pd.read_csv('/content/drive/MyDrive/bitpredict/final.csv',index_col=0)\n",
        "  print('df_csv.columns',df_csv.columns,' len(df_csv.columns) ',len(df_csv.columns))\n",
        "  li_data = list(final_df.loc[this_hour])\n",
        "  \n",
        "  for i in range(len(li_data)):\n",
        "    li_data[i] = round(float(li_data[i]),4)\n",
        "    \n",
        "  print(len(li_data),len(df_csv.columns),li_data)\n",
        "  index = this_hour.strftime('%d-%m-%Y %H:%M')\n",
        "  print('index',index)\n",
        "  \n",
        "  df_csv.loc[index] = li_data\n",
        "\n",
        "  print('df_csv.columns',df_csv.columns,' len(df_csv.columns) ',len(df_csv.columns))\n",
        "  if len(df_csv.columns)==23:\n",
        "    df_csv.to_csv('final.csv')\n",
        "    !cp final.csv \"drive/My Drive/bitpredict\"\n",
        "  \n",
        "  org_df = df_csv\n",
        "  print(org_df.shape)\n",
        "  \n",
        "  print(df_csv.iloc[-1])\n",
        "  \n",
        "  org_df['flair'] = org_df['gnews_flair'] + org_df['reddit_flair']\n",
        "  org_df['tb_polarity'] = org_df['gnews_tb_polarity'] + org_df['reddit_tb_polarity']\n",
        "  org_df['tb_subjectivity'] = org_df['gnews_tb_subjectivity'] + org_df['reddit_tb_subjectivity']\n",
        "  org_df['sid_pos'] = org_df['gnews_sid_pos'] + org_df['reddit_sid_pos']\n",
        "  org_df['sid_neg'] = org_df['gnews_sid_neg'] + org_df['reddit_sid_neg']\n",
        "  org_df = org_df[['open_BTCUSDT','high_BTCUSDT','low_BTCUSDT', 'close_BTCUSDT', 'volume_BTCUSDT', 'close_LTCUSD', 'volume_LTCUSD', 'close_ETHUSD', 'volume_ETHUSD', 'flair', 'tb_polarity', 'tb_subjectivity', 'sid_pos', 'sid_neg']]\n",
        "  \n",
        "  print(org_df.iloc[-1])\n",
        "  print(org_df.shape)\n",
        "  \n",
        "  dataset = org_df.values\n",
        "  dataset = dataset.astype('float32')\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  dataset = scaler.fit_transform(dataset)\n",
        "  \n",
        "  num_of_features = len(org_df.columns)\n",
        "  print('Number of features', num_of_features)\n",
        "  \n",
        "  expr_name = 'expr_4'\n",
        "  look_back = 24*15 \n",
        "  lstm_layers = 64\n",
        "  epochs=5\n",
        "  batch_size=128\n",
        "  \n",
        "  train_size_percent = 0.80\n",
        "  pred_col = org_df.columns.get_loc('close_BTCUSDT')\n",
        "  \n",
        "  # def create_dataset(dataset, pred_col, look_back=1):\n",
        "  #   dataX, dataY = [], []\n",
        "    \n",
        "  #   for i in range(len(dataset)-look_back):\n",
        "  #     a = dataset[i:(i+look_back), :]\n",
        "  #     dataX.append(a)\n",
        "  #     dataY.append(dataset[i + look_back, pred_col])\n",
        "  #   return np.array(dataX), np.array(dataY)\n",
        "  \n",
        "  train_size = int(len(dataset) * train_size_percent)\n",
        "  test_size = len(dataset) - train_size\n",
        "  train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
        "  print('train:',train)\n",
        "  \n",
        "  trainX, trainY = create_dataset(train, pred_col, look_back=look_back)\n",
        "  testX, testY = create_dataset(test, pred_col, look_back=look_back)\n",
        "  \n",
        "  trainX = np.reshape(trainX, (trainX.shape[0], look_back, num_of_features))\n",
        "  testX = np.reshape(testX, (testX.shape[0],look_back, num_of_features))\n",
        "  \n",
        "  print('Training dataset length ', len(train))\n",
        "  print('Testing dataset length ', len(test))\n",
        "  print('look_back ', look_back)\n",
        "  print('total data',dataset.shape,' ',len(train)+len(test))\n",
        "  model = keras.models.load_model('/content/drive/MyDrive/bitpredict/model_l15_e8_train_last_80.h5')\n",
        "  \n",
        "  inp = testX[-10][newaxis,:,:]\n",
        "  print(inp.shape)\n",
        "  \n",
        "  testPredict = model.predict(inp)\n",
        "  print(testPredict,testPredict.shape)\n",
        "  \n",
        "  testPredict_extended = np.zeros((len(testPredict),num_of_features))\n",
        "  \n",
        "  testPredict_extended[:,pred_col] = testPredict[:,0]\n",
        "  testPredict = scaler.inverse_transform(testPredict_extended)[:,pred_col]\n",
        "  \n",
        "  testY_extended = np.zeros((len(testY),num_of_features))\n",
        "  testY_extended[:,pred_col]=testY\n",
        "  testY = scaler.inverse_transform(testY_extended)[:,pred_col]\n",
        "  \n",
        "  print('prediction:',testPredict)\n",
        "  print('Actual',testY[-10])\n",
        "  \n",
        "    \n",
        "  print(testPredict)\n",
        "  print('Actual',testY[-1:])\n",
        "  \n",
        "  db_hour = str(int(time.mktime(this_hour.timetuple())))\n",
        "  pprize = testPredict[0]\n",
        "  oprize = testY[-1]\n",
        "  \n",
        "  \n",
        "  if not firebase_admin._apps:\n",
        "    \n",
        "    cred_obj = credentials.Certificate('/content/drive/MyDrive/bitpredict/bitcoin-price-predictor-firebase-adminsdk-erp3v-7e3aceca37.json')\n",
        "    \n",
        "    default_app = firebase_admin.initialize_app(cred_obj, {\n",
        "        'projectId': 'bitcoin-price-predictor',\n",
        "        })\n",
        "    \n",
        "  pprize = round(pprize,4)\n",
        "  oprize = round(oprize,4)\n",
        "  \n",
        "  realtimedb = {\n",
        "          'timestamp':db_hour,\n",
        "          'predicted':oprize,\n",
        "          'last':pprize\n",
        "  }\n",
        "  \n",
        "  db = firestore.client()\n",
        "  doc_ref = db.collection(u'bitsmaptrimmed')\n",
        "  doc_ref.add(realtimedb)\n",
        "  print(realtimedb)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPJ7Dszio9aU"
      },
      "source": [
        "### **run file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQtvbhazk8Yo",
        "outputId": "1676a9ec-7626-4492-b132-2e1008c681a0"
      },
      "source": [
        "# run_script()\n",
        "# import datetime\n",
        "# import time\n",
        "# # filename='/content/drive/MyDrive/bitpredict/pipelining.py'\n",
        "\n",
        "\n",
        "\n",
        "while(1):\n",
        "\n",
        "  try:\n",
        "    d = datetime.datetime.now().minute\n",
        "    if d==0:\n",
        "      run_script()\n",
        "      f = open('/content/drive/MyDrive/bitpredict/log.txt', 'a')\n",
        "      print(str(datetime.datetime.now()) + \" EXECUTED \")\n",
        "      f.write(str(datetime.datetime.now()) + \" EXECUTED\" )\n",
        "      f.close()\n",
        "    else:\n",
        "      print(datetime.datetime.now(),' waiting')\n",
        "      time.sleep(50)\n",
        "\n",
        "  except Exception as e:\n",
        "    f = open('/content/drive/MyDrive/bitpredict/log.txt', 'a')\n",
        "    print(str(datetime.datetime.now()) + \" NOT EXECUTED \"+ str(e))\n",
        "    f.write(str(datetime.datetime.now()) + \" NOT EXECUTED \"+ str(e))\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # try:\n",
        "    #     with open(filename, \"rb\") as source_file:\n",
        "    #         code = compile(source_file.read(), filename, \"exec\")\n",
        "\n",
        "    #     d = datetime.datetime.now().minute\n",
        "       \n",
        "\n",
        "    #     # print(d)\n",
        "    #     #print(d,type(d))\n",
        "    #     if d==8:\n",
        "    #         exec(code)\n",
        "    #         f = open('/content/drive/MyDrive/bitpredict/log.txt', 'a')\n",
        "    #         f.write(str(datetime.datetime.now())+ \" EXECUTED\\n \")\n",
        "    #         f.close()\n",
        "    #     else:\n",
        "    #         time.sleep(55)\n",
        "\n",
        "    # except Exception as e:\n",
        "    #     f = open('/content/drive/MyDrive/bitpredict/log.txt', 'a')\n",
        "    #     print()\n",
        "    #     f.write(str(datetime.datetime.now()) + \" NOT EXECUTED \"+ str(e))\n",
        "    #     f.close()\n",
        "\n",
        "\n",
        "# exec(code, globals, locals)\n",
        "# # exec(compile(open('pipelining.py', \"rb\").read(), 'pipelining.py', 'exec'))\n",
        "# print('this\\nankur')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-10 05:57:52.255414  waiting\n",
            "2021-10-10 05:58:42.298663  waiting\n",
            "2021-10-10 05:59:32.306052  waiting\n",
            "Time Now  2021-10-10 06:00:22.312373\n",
            "Last Hour 2021-10-10 05:00:00\n",
            "date_yday  2021-10-09\n",
            "this hour  2021-10-10 06:00:00\n",
            "https://api.pushshift.io/reddit/search/submission/?title=bitcoin&size=1000&after=1633842000&subreddit=bitcoin\n",
            "reddit data  [{'all_awardings': [], 'allow_live_comments': False, 'author': 'Teacherinthestreets', 'author_flair_css_class': 'noob', 'author_flair_richtext': [{'e': 'text', 't': 'redditor for 4 weeks'}], 'author_flair_template_id': '2ec8e69e-6c36-11e9-a04b-0afb553d4ea6', 'author_flair_text': 'redditor for 4 weeks', 'author_flair_text_color': 'dark', 'author_flair_type': 'richtext', 'author_fullname': 't2_ef7wmpjh', 'author_is_blocked': False, 'author_patreon_flair': False, 'author_premium': False, 'awarders': [], 'can_mod_post': False, 'contest_mode': False, 'created_utc': 1633843193, 'domain': 'youtube.com', 'full_link': 'https://www.reddit.com/r/Bitcoin/comments/q51ra9/bitcoin_tarot_reading_for_fun/', 'gildings': {}, 'id': 'q51ra9', 'is_created_from_ads_ui': False, 'is_crosspostable': True, 'is_meta': False, 'is_original_content': False, 'is_reddit_media_domain': False, 'is_robot_indexable': True, 'is_self': False, 'is_video': False, 'link_flair_background_color': '', 'link_flair_richtext': [], 'link_flair_text_color': 'dark', 'link_flair_type': 'text', 'locked': False, 'media_only': False, 'no_follow': True, 'num_comments': 0, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/Bitcoin/comments/q51ra9/bitcoin_tarot_reading_for_fun/', 'pinned': False, 'post_hint': 'link', 'preview': {'enabled': False, 'images': [{'id': 'ZR0RV_IQ3oSYR5gTfHsks56wUFzk_axgHMpP2XifK4o', 'resolutions': [{'height': 81, 'url': 'https://external-preview.redd.it/pTOXB6ZHDnlhakHOFrR_DEi4uPgbkXx_pePYO54-g4U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f523258dfc20abf7047147d8b382813a9f39e20e', 'width': 108}, {'height': 162, 'url': 'https://external-preview.redd.it/pTOXB6ZHDnlhakHOFrR_DEi4uPgbkXx_pePYO54-g4U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2490822af10e45ddcce134c7b5eb39ee9594c89', 'width': 216}, {'height': 240, 'url': 'https://external-preview.redd.it/pTOXB6ZHDnlhakHOFrR_DEi4uPgbkXx_pePYO54-g4U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47da3845dffc3fb46d38393567a984b65d33dd49', 'width': 320}], 'source': {'height': 360, 'url': 'https://external-preview.redd.it/pTOXB6ZHDnlhakHOFrR_DEi4uPgbkXx_pePYO54-g4U.jpg?auto=webp&amp;s=3ba1280a4671e160919b51b558e38a5d68ead6ed', 'width': 480}, 'variants': {}}]}, 'pwls': 6, 'retrieved_on': 1633843204, 'score': 1, 'selftext': '', 'send_replies': True, 'spoiler': False, 'stickied': False, 'subreddit': 'Bitcoin', 'subreddit_id': 't5_2s3qj', 'subreddit_subscribers': 3418744, 'subreddit_type': 'public', 'thumbnail': 'https://a.thumbs.redditmedia.com/DTFIRoHd5ZsppphlrqHIwFCvOaCEqzWdHLuWsx41mb0.jpg', 'thumbnail_height': 105, 'thumbnail_width': 140, 'title': 'Bitcoin Tarot reading for fun', 'total_awards_received': 0, 'treatment_tags': [], 'upvote_ratio': 1.0, 'url': 'https://youtube.com/shorts/pHJ6JiExmv8?feature=share', 'url_overridden_by_dest': 'https://youtube.com/shorts/pHJ6JiExmv8?feature=share', 'whitelist_status': 'all_ads', 'wls': 6}, {'all_awardings': [], 'allow_live_comments': False, 'author': 'Key-Floor-3550', 'author_flair_css_class': None, 'author_flair_richtext': [], 'author_flair_text': None, 'author_flair_type': 'text', 'author_fullname': 't2_9i0veua6', 'author_is_blocked': False, 'author_patreon_flair': False, 'author_premium': False, 'awarders': [], 'can_mod_post': False, 'contest_mode': False, 'created_utc': 1633844040, 'domain': 'self.Bitcoin', 'full_link': 'https://www.reddit.com/r/Bitcoin/comments/q51xnk/quit_stocks_to_get_into_bitcoin/', 'gildings': {}, 'id': 'q51xnk', 'is_created_from_ads_ui': False, 'is_crosspostable': True, 'is_meta': False, 'is_original_content': False, 'is_reddit_media_domain': False, 'is_robot_indexable': True, 'is_self': True, 'is_video': False, 'link_flair_background_color': '', 'link_flair_richtext': [], 'link_flair_text_color': 'dark', 'link_flair_type': 'text', 'locked': False, 'media_only': False, 'no_follow': True, 'num_comments': 0, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/Bitcoin/comments/q51xnk/quit_stocks_to_get_into_bitcoin/', 'pinned': False, 'pwls': 6, 'retrieved_on': 1633844050, 'score': 1, 'selftext': \"The thing that has made me distant towards bitcoin was how mysterious it seemed, FUD, and so.\\n\\nThe returns of bitcoin however are insane compared to stocks. and even if you get into a bad position if you wait long enough you can be sure to make money in the long run. So it seems like a safe investment.\\n\\nWhat is the catch? it is commonly known high returns are associated with high risk. But bitcoin doesn't seem high risk on the long term side of things, short term definitely. but I haven't found any other better investment option.\", 'send_replies': True, 'spoiler': False, 'stickied': False, 'subreddit': 'Bitcoin', 'subreddit_id': 't5_2s3qj', 'subreddit_subscribers': 3418758, 'subreddit_type': 'public', 'thumbnail': 'self', 'title': 'Quit stocks to get into bitcoin', 'total_awards_received': 0, 'treatment_tags': [], 'upvote_ratio': 1.0, 'url': 'https://www.reddit.com/r/Bitcoin/comments/q51xnk/quit_stocks_to_get_into_bitcoin/', 'whitelist_status': 'all_ads', 'wls': 6}]\n",
            "count  0\n",
            "2\n",
            "2021-10-10 05:34:00\n",
            "https://api.pushshift.io/reddit/search/submission/?title=bitcoin&size=1000&after=1633844040&subreddit=bitcoin\n",
            "Index(['post_id', 'title', 'selftext', 'url', 'author', 'score',\n",
            "       'publish_date', 'num_of_comments', 'permalink', 'flair'],\n",
            "      dtype='object')\n",
            "2021-10-10 05:19:53\n",
            "Bitcoin Tarot r\n",
            "[POSITIVE (0.9788)]\n",
            " 0.9788\n",
            "2021-10-10 05:34:00\n",
            "Quit stocks to \n",
            "[NEGATIVE (0.9799)]\n",
            " 0.9799\n",
            "Ankur: end\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "2021-10-09\n",
            "this is url: https://www.google.com/search?q=cryptocurrency+bitcoin+after%3A2021-10-08+before%3A2021-10-09&hl=en&biw=1536&bih=731&tbs=sbd%3A1&tbm=nws&sxsrf=AOaemvJvsALAxFs9-Zs_bHUW8HthpSttsw%3A1631421050994&ei=eoI9YZicPOCUr7wP9MGg0A8&oq=cryptocurrency+after%3A2021-10-08+before%3A2021-10-09&gs_l=psy-ab.3...5565.5565.0.6063.1.1.0.0.0.0.156.156.0j1.1.0....0...1c.1.64.psy-ab..0.0.0....0.e0I-wakyIOk\n",
            "<class 'datetime.date'>\n",
            "2021-10-09\n",
            "2021-10-10\n",
            "this is url: https://www.google.com/search?q=cryptocurrency+bitcoin+after%3A2021-10-09+before%3A2021-10-10&hl=en&biw=1536&bih=731&tbs=sbd%3A1&tbm=nws&sxsrf=AOaemvJvsALAxFs9-Zs_bHUW8HthpSttsw%3A1631421050994&ei=eoI9YZicPOCUr7wP9MGg0A8&oq=cryptocurrency+after%3A2021-10-09+before%3A2021-10-10&gs_l=psy-ab.3...5565.5565.0.6063.1.1.0.0.0.0.156.156.0j1.1.0....0...1c.1.64.psy-ab..0.0.0....0.e0I-wakyIOk\n",
            "<class 'datetime.date'>\n",
            "2021-10-10\n",
            "2021-10-10 06:00:46,766 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
            "[POSITIVE (0.9311)]\n",
            " 0.9311\n",
            "[NEGATIVE (0.9954)]\n",
            " 0.9954\n",
            "[NEGATIVE (0.9988)]\n",
            " 0.9988\n",
            "[NEGATIVE (0.9948)]\n",
            " 0.9948\n",
            "[NEGATIVE (0.9981)]\n",
            " 0.9981\n",
            "[NEGATIVE (0.5742)]\n",
            " 0.5742\n",
            "[NEGATIVE (0.9513)]\n",
            " 0.9513\n",
            "[NEGATIVE (0.9924)]\n",
            " 0.9924\n",
            "[NEGATIVE (0.9994)]\n",
            " 0.9994\n",
            "2021-10-09   1\n",
            "-0.7303666666666666 0.10141624534560165 0.3917906177533448\n",
            "[POSITIVE (0.7737)]\n",
            " 0.7737\n",
            "[NEGATIVE (0.9995)]\n",
            " 0.9995\n",
            "[NEGATIVE (0.9999)]\n",
            " 0.9999\n",
            "[NEGATIVE (0.9828)]\n",
            " 0.9828\n",
            "[NEGATIVE (0.7749)]\n",
            " 0.7749\n",
            "[NEGATIVE (1.0)]\n",
            " 1.0\n",
            "[NEGATIVE (1.0)]\n",
            " 1.0\n",
            "[NEGATIVE (0.9997)]\n",
            " 0.9997\n",
            "[NEGATIVE (0.7154)]\n",
            " 0.7154\n",
            "2021-10-10   2\n",
            "-0.7442777777777777 0.12284602773091893 0.4069902306755837\n",
            "------------------------------------------------------------\n",
            "Downloading data from 2021-10-10 05:00:00 to 2021-10-10 06:00:00 for LTCUSDT\n",
            "------------------------------------------------------------\n",
            "Step 0:Downloading data from 2021-10-10 05:00:00 to 2021-10-11 05:00:00\n",
            "\t Downloaded data of len 2 from 2021-10-10 05:00:00 to 2021-10-11 05:00:00\n",
            "------------------------------------------------------------\n",
            "Downloading data from 2021-10-10 05:00:00 to 2021-10-10 06:00:00 for ETHUSDT\n",
            "------------------------------------------------------------\n",
            "Step 0:Downloading data from 2021-10-10 05:00:00 to 2021-10-11 05:00:00\n",
            "\t Downloaded data of len 2 from 2021-10-10 05:00:00 to 2021-10-11 05:00:00\n",
            "------------------------------------------------------------\n",
            "Downloading data from 2021-10-10 05:00:00 to 2021-10-10 06:00:00 for BTCUSDT\n",
            "------------------------------------------------------------\n",
            "Step 0:Downloading data from 2021-10-10 05:00:00 to 2021-10-11 05:00:00\n",
            "\t Downloaded data of len 2 from 2021-10-10 05:00:00 to 2021-10-11 05:00:00\n",
            "enu <enumerate object at 0x7f672bce2460>\n",
            "df_csv.columns Index(['open_BTCUSDT', 'high_BTCUSDT', 'low_BTCUSDT', 'close_BTCUSDT',\n",
            "       'volume_BTCUSDT', 'close_LTCUSD', 'volume_LTCUSD', 'close_ETHUSD',\n",
            "       'volume_ETHUSD', 'gnews_flair', 'gnews_tb_polarity',\n",
            "       'gnews_tb_subjectivity', 'gnews_sid_pos', 'gnews_sid_neg',\n",
            "       'gnews_sid_neu', 'gnews_sid_com', 'reddit_flair', 'reddit_tb_polarity',\n",
            "       'reddit_tb_subjectivity', 'reddit_sid_pos', 'reddit_sid_neg',\n",
            "       'reddit_sid_neu', 'reddit_sid_com'],\n",
            "      dtype='object')  len(df_csv.columns)  23\n",
            "23 23 [55651.72, 55706.8, 55513.8, 55544.47, 125.7865, 182.6, 467.415, 3563.98, 453.3887, -0.7443, 0.1228, 0.407, 0.0993, 0.0601, 0.8403, 0.4821, -0.0005, 0.139, 0.375, 0.276, 0.0405, 0.6835, 0.4906]\n",
            "index 10-10-2021 06:00\n",
            "df_csv.columns Index(['open_BTCUSDT', 'high_BTCUSDT', 'low_BTCUSDT', 'close_BTCUSDT',\n",
            "       'volume_BTCUSDT', 'close_LTCUSD', 'volume_LTCUSD', 'close_ETHUSD',\n",
            "       'volume_ETHUSD', 'gnews_flair', 'gnews_tb_polarity',\n",
            "       'gnews_tb_subjectivity', 'gnews_sid_pos', 'gnews_sid_neg',\n",
            "       'gnews_sid_neu', 'gnews_sid_com', 'reddit_flair', 'reddit_tb_polarity',\n",
            "       'reddit_tb_subjectivity', 'reddit_sid_pos', 'reddit_sid_neg',\n",
            "       'reddit_sid_neu', 'reddit_sid_com'],\n",
            "      dtype='object')  len(df_csv.columns)  23\n",
            "(33042, 23)\n",
            "open_BTCUSDT              55651.7200\n",
            "high_BTCUSDT              55706.8000\n",
            "low_BTCUSDT               55513.8000\n",
            "close_BTCUSDT             55544.4700\n",
            "volume_BTCUSDT              125.7865\n",
            "close_LTCUSD                182.6000\n",
            "volume_LTCUSD               467.4150\n",
            "close_ETHUSD               3563.9800\n",
            "volume_ETHUSD               453.3887\n",
            "gnews_flair                  -0.7443\n",
            "gnews_tb_polarity             0.1228\n",
            "gnews_tb_subjectivity         0.4070\n",
            "gnews_sid_pos                 0.0993\n",
            "gnews_sid_neg                 0.0601\n",
            "gnews_sid_neu                 0.8403\n",
            "gnews_sid_com                 0.4821\n",
            "reddit_flair                 -0.0005\n",
            "reddit_tb_polarity            0.1390\n",
            "reddit_tb_subjectivity        0.3750\n",
            "reddit_sid_pos                0.2760\n",
            "reddit_sid_neg                0.0405\n",
            "reddit_sid_neu                0.6835\n",
            "reddit_sid_com                0.4906\n",
            "Name: 10-10-2021 06:00, dtype: float64\n",
            "open_BTCUSDT       55651.7200\n",
            "high_BTCUSDT       55706.8000\n",
            "low_BTCUSDT        55513.8000\n",
            "close_BTCUSDT      55544.4700\n",
            "volume_BTCUSDT       125.7865\n",
            "close_LTCUSD         182.6000\n",
            "volume_LTCUSD        467.4150\n",
            "close_ETHUSD        3563.9800\n",
            "volume_ETHUSD        453.3887\n",
            "flair                 -0.7448\n",
            "tb_polarity            0.2618\n",
            "tb_subjectivity        0.7820\n",
            "sid_pos                0.3753\n",
            "sid_neg                0.1006\n",
            "Name: 10-10-2021 06:00, dtype: float64\n",
            "(33042, 14)\n",
            "Number of features 14\n",
            "train: [[0.17383705 0.17285925 0.16959345 ... 0.21776645 0.06050594 0.03008615]\n",
            " [0.16956633 0.16905692 0.16554542 ... 0.39153585 0.15866031 0.1396786 ]\n",
            " [0.16476418 0.1660478  0.16449101 ... 0.40180603 0.09024265 0.05314778]\n",
            " ...\n",
            " [0.5206187  0.5201644  0.5105728  ... 0.4175536  0.18298401 0.03761432]\n",
            " [0.51315266 0.51533675 0.50509477 ... 0.5110052  0.17651008 0.04353877]\n",
            " [0.5120015  0.50977623 0.49434954 ... 0.3801381  0.21941146 0.04512922]]\n",
            "Training dataset length  26433\n",
            "Testing dataset length  6609\n",
            "look_back  360\n",
            "total data (33042, 14)   33042\n",
            "(1, 360, 14)\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f672b7e19e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[0.83579504]] (1, 1)\n",
            "prediction: [54488.99569962]\n",
            "Actual 54293.88794205278\n",
            "[54488.99569962]\n",
            "Actual [55544.4671476]\n",
            "{'timestamp': '1633845600', 'predicted': 55544.4671, 'last': 54488.9957}\n",
            "2021-10-10 06:01:09.782438 EXECUTED \n",
            "2021-10-10 06:01:09.784416  waiting\n",
            "2021-10-10 06:01:59.835720  waiting\n",
            "2021-10-10 06:02:49.867699  waiting\n",
            "2021-10-10 06:03:39.895960  waiting\n",
            "2021-10-10 06:04:29.912971  waiting\n",
            "2021-10-10 06:05:19.963283  waiting\n",
            "2021-10-10 06:06:10.003198  waiting\n",
            "2021-10-10 06:07:00.012984  waiting\n",
            "2021-10-10 06:07:50.026088  waiting\n",
            "2021-10-10 06:08:40.030002  waiting\n",
            "2021-10-10 06:09:30.040503  waiting\n",
            "2021-10-10 06:10:20.090937  waiting\n",
            "2021-10-10 06:11:10.121243  waiting\n",
            "2021-10-10 06:12:00.171608  waiting\n",
            "2021-10-10 06:12:50.208983  waiting\n",
            "2021-10-10 06:13:40.237871  waiting\n",
            "2021-10-10 06:14:30.247523  waiting\n",
            "2021-10-10 06:15:20.298048  waiting\n",
            "2021-10-10 06:16:10.312032  waiting\n",
            "2021-10-10 06:17:00.363131  waiting\n",
            "2021-10-10 06:17:50.414215  waiting\n",
            "2021-10-10 06:18:40.431902  waiting\n",
            "2021-10-10 06:19:30.460004  waiting\n",
            "2021-10-10 06:20:20.504574  waiting\n",
            "2021-10-10 06:21:10.519259  waiting\n",
            "2021-10-10 06:22:00.522877  waiting\n",
            "2021-10-10 06:22:50.562061  waiting\n",
            "2021-10-10 06:23:40.607172  waiting\n",
            "2021-10-10 06:24:30.621061  waiting\n",
            "2021-10-10 06:25:20.645519  waiting\n",
            "2021-10-10 06:26:10.681385  waiting\n",
            "2021-10-10 06:27:00.683596  waiting\n",
            "2021-10-10 06:27:50.709059  waiting\n",
            "2021-10-10 06:28:40.752012  waiting\n",
            "2021-10-10 06:29:30.802294  waiting\n",
            "2021-10-10 06:30:20.827840  waiting\n",
            "2021-10-10 06:31:10.858059  waiting\n",
            "2021-10-10 06:32:00.876242  waiting\n",
            "2021-10-10 06:32:50.922035  waiting\n",
            "2021-10-10 06:33:40.959813  waiting\n",
            "2021-10-10 06:34:30.960308  waiting\n",
            "2021-10-10 06:35:21.002956  waiting\n",
            "2021-10-10 06:36:11.037187  waiting\n",
            "2021-10-10 06:37:01.083426  waiting\n",
            "2021-10-10 06:37:51.129188  waiting\n",
            "2021-10-10 06:38:41.166602  waiting\n",
            "2021-10-10 06:39:31.202511  waiting\n",
            "2021-10-10 06:40:21.209431  waiting\n",
            "2021-10-10 06:41:11.259795  waiting\n",
            "2021-10-10 06:42:01.310098  waiting\n",
            "2021-10-10 06:42:51.345790  waiting\n",
            "2021-10-10 06:43:41.379343  waiting\n",
            "2021-10-10 06:44:31.409149  waiting\n",
            "2021-10-10 06:45:21.459441  waiting\n",
            "2021-10-10 06:46:11.479058  waiting\n",
            "2021-10-10 06:47:01.529337  waiting\n",
            "2021-10-10 06:47:51.554116  waiting\n",
            "2021-10-10 06:48:41.576505  waiting\n",
            "2021-10-10 06:49:31.626067  waiting\n",
            "2021-10-10 06:50:21.676452  waiting\n",
            "2021-10-10 06:51:11.709844  waiting\n",
            "2021-10-10 06:52:01.753013  waiting\n",
            "2021-10-10 06:52:51.761388  waiting\n",
            "2021-10-10 06:53:41.783392  waiting\n",
            "2021-10-10 06:54:31.786095  waiting\n",
            "2021-10-10 06:55:21.836366  waiting\n",
            "2021-10-10 06:56:11.842010  waiting\n",
            "2021-10-10 06:57:01.863641  waiting\n",
            "2021-10-10 06:57:51.911418  waiting\n",
            "2021-10-10 06:58:41.961368  waiting\n",
            "2021-10-10 06:59:32.011472  waiting\n",
            "Time Now  2021-10-10 07:00:22.029186\n",
            "Last Hour 2021-10-10 06:00:00\n",
            "date_yday  2021-10-09\n",
            "this hour  2021-10-10 07:00:00\n",
            "https://api.pushshift.io/reddit/search/submission/?title=bitcoin&size=1000&after=1633845600&subreddit=bitcoin\n",
            "reddit data  [{'all_awardings': [], 'allow_live_comments': False, 'author': 'HabileJ_6', 'author_flair_css_class': None, 'author_flair_richtext': [], 'author_flair_text': None, 'author_flair_type': 'text', 'author_fullname': 't2_av5ffx20', 'author_is_blocked': False, 'author_patreon_flair': False, 'author_premium': False, 'awarders': [], 'can_mod_post': False, 'contest_mode': False, 'created_utc': 1633846212, 'domain': 'thecryptobasic.com', 'full_link': 'https://www.reddit.com/r/Bitcoin/comments/q52ds5/president_nayib_bukele_says_el_salvador_will_use/', 'gildings': {}, 'id': 'q52ds5', 'is_created_from_ads_ui': False, 'is_crosspostable': True, 'is_meta': False, 'is_original_content': False, 'is_reddit_media_domain': False, 'is_robot_indexable': True, 'is_self': False, 'is_video': False, 'link_flair_background_color': '', 'link_flair_richtext': [], 'link_flair_text_color': 'dark', 'link_flair_type': 'text', 'locked': False, 'media_only': False, 'no_follow': True, 'num_comments': 0, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/Bitcoin/comments/q52ds5/president_nayib_bukele_says_el_salvador_will_use/', 'pinned': False, 'post_hint': 'link', 'preview': {'enabled': False, 'images': [{'id': 'ENj9KEdbGCpEZsFUCu3EXUGJnQg9L6LTvzg13fYX9H0', 'resolutions': [{'height': 56, 'url': 'https://external-preview.redd.it/DuBiXISlTjo7EA0zhj1gwBYxxVqWuKyAELa8QMtr9uw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1362af99df23bd3ab338555374666be6dc0db3bd', 'width': 108}, {'height': 113, 'url': 'https://external-preview.redd.it/DuBiXISlTjo7EA0zhj1gwBYxxVqWuKyAELa8QMtr9uw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1033dc4cbcdcf5103e690ae1a3e01796f5d3e611', 'width': 216}, {'height': 168, 'url': 'https://external-preview.redd.it/DuBiXISlTjo7EA0zhj1gwBYxxVqWuKyAELa8QMtr9uw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2475cc9978506f2df60b7717badbe0b43e4797c', 'width': 320}, {'height': 337, 'url': 'https://external-preview.redd.it/DuBiXISlTjo7EA0zhj1gwBYxxVqWuKyAELa8QMtr9uw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f82fd7e5b6f4af328090afd4ae6771b6c9af4047', 'width': 640}], 'source': {'height': 457, 'url': 'https://external-preview.redd.it/DuBiXISlTjo7EA0zhj1gwBYxxVqWuKyAELa8QMtr9uw.jpg?auto=webp&amp;s=17e2599adeab4ae583fd1325e3113427d46ee9ee', 'width': 867}, 'variants': {}}]}, 'pwls': 6, 'retrieved_on': 1633846223, 'score': 1, 'selftext': '', 'send_replies': True, 'spoiler': False, 'stickied': False, 'subreddit': 'Bitcoin', 'subreddit_id': 't5_2s3qj', 'subreddit_subscribers': 3418802, 'subreddit_type': 'public', 'thumbnail': 'https://b.thumbs.redditmedia.com/pVkhsFYzFFOJtUGAV3RV8NxrLZzsoaZXgbCMT0IBv3A.jpg', 'thumbnail_height': 73, 'thumbnail_width': 140, 'title': 'President Nayib Bukele Says El Salvador Will Use Bitcoin Profits To Fund Pet Hospital', 'total_awards_received': 0, 'treatment_tags': [], 'upvote_ratio': 1.0, 'url': 'https://thecryptobasic.com/2021/10/10/president-nayib-bukele-says-el-salvador-will-use-bitcoin-profits-to-fund-pet-hospital/', 'url_overridden_by_dest': 'https://thecryptobasic.com/2021/10/10/president-nayib-bukele-says-el-salvador-will-use-bitcoin-profits-to-fund-pet-hospital/', 'whitelist_status': 'all_ads', 'wls': 6}, {'all_awardings': [], 'allow_live_comments': False, 'author': 'JunoKat', 'author_flair_css_class': None, 'author_flair_richtext': [], 'author_flair_text': None, 'author_flair_type': 'text', 'author_fullname': 't2_5wsfhb4z', 'author_is_blocked': False, 'author_patreon_flair': False, 'author_premium': False, 'awarders': [], 'can_mod_post': False, 'contest_mode': False, 'created_utc': 1633847159, 'domain': 'reddit.com', 'full_link': 'https://www.reddit.com/r/Bitcoin/comments/q52kuw/lets_tweet_at_wikipedia_to_add_bitcoin_lightning/', 'gallery_data': {'items': [{'caption': 'Why pay £0.4 for £10 of donation?', 'id': 77569759, 'media_id': 'honl8oexgks71'}, {'caption': 'If you were to add lightning, I would donate asap', 'id': 77569760, 'media_id': 'dco77nexgks71'}]}, 'gildings': {}, 'id': 'q52kuw', 'is_created_from_ads_ui': False, 'is_crosspostable': True, 'is_gallery': True, 'is_meta': False, 'is_original_content': False, 'is_reddit_media_domain': False, 'is_robot_indexable': True, 'is_self': False, 'is_video': False, 'link_flair_background_color': '', 'link_flair_richtext': [], 'link_flair_text_color': 'dark', 'link_flair_type': 'text', 'locked': False, 'media_metadata': {'dco77nexgks71': {'e': 'Image', 'id': 'dco77nexgks71', 'm': 'image/jpg', 'p': [{'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac91228f5d4acec6bad9ca59025d3f924a85f4fa', 'x': 108, 'y': 216}, {'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fbd05c1bbdca364a3f3a8826ee76d52dfad968a3', 'x': 216, 'y': 432}, {'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=534a6760fb2435bc80cce25f9538a19111afbc36', 'x': 320, 'y': 640}, {'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb4eb3d31ba6e90b4034950a1a62b01cb27f60b8', 'x': 640, 'y': 1280}, {'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=955a527552089b0e3df900786a89d49ae38ed360', 'x': 960, 'y': 1920}, {'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3cc8fae0115a01bb0b01399124799abfd96871a', 'x': 1080, 'y': 2160}], 's': {'u': 'https://preview.redd.it/dco77nexgks71.jpg?width=1125&amp;format=pjpg&amp;auto=webp&amp;s=8b6fd6a7f97c375e192a6bf5d5e96c8bf1fe837c', 'x': 1125, 'y': 2436}, 'status': 'valid'}, 'honl8oexgks71': {'e': 'Image', 'id': 'honl8oexgks71', 'm': 'image/jpg', 'p': [{'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8163531ce9844c39f58bcabe26aae20235a8091c', 'x': 108, 'y': 216}, {'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2297d4a1c7defd6fbee42a53ccbd30673a7f841d', 'x': 216, 'y': 432}, {'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69af53d5e6524d0f267cde2925f5f5d2d7bbb396', 'x': 320, 'y': 640}, {'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=700d5cf659eac4a43d85e26e54e2bf04282cdb3a', 'x': 640, 'y': 1280}, {'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=72826bacb5f187c4f3281c6f5db52c372988cc5d', 'x': 960, 'y': 1920}, {'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98fb940bb5ebc5f751150abf2dd10e445aadf741', 'x': 1080, 'y': 2160}], 's': {'u': 'https://preview.redd.it/honl8oexgks71.jpg?width=1125&amp;format=pjpg&amp;auto=webp&amp;s=8714da8ce35a6d65538fea359fd53ca6d7368326', 'x': 1125, 'y': 2436}, 'status': 'valid'}}, 'media_only': False, 'no_follow': False, 'num_comments': 0, 'num_crossposts': 0, 'over_18': False, 'parent_whitelist_status': 'all_ads', 'permalink': '/r/Bitcoin/comments/q52kuw/lets_tweet_at_wikipedia_to_add_bitcoin_lightning/', 'pinned': False, 'pwls': 6, 'retrieved_on': 1633847169, 'score': 1, 'selftext': '', 'send_replies': True, 'spoiler': False, 'stickied': False, 'subreddit': 'Bitcoin', 'subreddit_id': 't5_2s3qj', 'subreddit_subscribers': 3418819, 'subreddit_type': 'public', 'thumbnail': 'https://b.thumbs.redditmedia.com/hCLrN84kS-v_ExrQ_XSfTVzEQPALe33EmcdccDL2q-s.jpg', 'thumbnail_height': 140, 'thumbnail_width': 140, 'title': 'Let’s tweet at Wikipedia to add Bitcoin lightning as a mode of donation', 'total_awards_received': 0, 'treatment_tags': [], 'upvote_ratio': 1.0, 'url': 'https://www.reddit.com/gallery/q52kuw', 'url_overridden_by_dest': 'https://www.reddit.com/gallery/q52kuw', 'whitelist_status': 'all_ads', 'wls': 6}]\n",
            "count  0\n",
            "2\n",
            "2021-10-10 06:25:59\n",
            "https://api.pushshift.io/reddit/search/submission/?title=bitcoin&size=1000&after=1633847159&subreddit=bitcoin\n",
            "Index(['post_id', 'title', 'selftext', 'url', 'author', 'score',\n",
            "       'publish_date', 'num_of_comments', 'permalink', 'flair'],\n",
            "      dtype='object')\n",
            "2021-10-10 06:10:12\n",
            "President Nayib\n",
            "[NEGATIVE (0.9834)]\n",
            " 0.9834\n",
            "2021-10-10 06:25:59\n",
            "Let’s tweet at \n",
            "[NEGATIVE (0.9938)]\n",
            " 0.9938\n",
            "Ankur: end\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "2021-10-09\n",
            "this is url: https://www.google.com/search?q=cryptocurrency+bitcoin+after%3A2021-10-08+before%3A2021-10-09&hl=en&biw=1536&bih=731&tbs=sbd%3A1&tbm=nws&sxsrf=AOaemvJvsALAxFs9-Zs_bHUW8HthpSttsw%3A1631421050994&ei=eoI9YZicPOCUr7wP9MGg0A8&oq=cryptocurrency+after%3A2021-10-08+before%3A2021-10-09&gs_l=psy-ab.3...5565.5565.0.6063.1.1.0.0.0.0.156.156.0j1.1.0....0...1c.1.64.psy-ab..0.0.0....0.e0I-wakyIOk\n",
            "<class 'datetime.date'>\n",
            "2021-10-09\n",
            "2021-10-10\n",
            "this is url: https://www.google.com/search?q=cryptocurrency+bitcoin+after%3A2021-10-09+before%3A2021-10-10&hl=en&biw=1536&bih=731&tbs=sbd%3A1&tbm=nws&sxsrf=AOaemvJvsALAxFs9-Zs_bHUW8HthpSttsw%3A1631421050994&ei=eoI9YZicPOCUr7wP9MGg0A8&oq=cryptocurrency+after%3A2021-10-09+before%3A2021-10-10&gs_l=psy-ab.3...5565.5565.0.6063.1.1.0.0.0.0.156.156.0j1.1.0....0...1c.1.64.psy-ab..0.0.0....0.e0I-wakyIOk\n",
            "<class 'datetime.date'>\n",
            "2021-10-10\n",
            "2021-10-10 07:00:42,641 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n",
            "[POSITIVE (0.9311)]\n",
            " 0.9311\n",
            "[NEGATIVE (0.9954)]\n",
            " 0.9954\n",
            "[NEGATIVE (0.9988)]\n",
            " 0.9988\n",
            "[NEGATIVE (0.9948)]\n",
            " 0.9948\n",
            "[NEGATIVE (0.9981)]\n",
            " 0.9981\n",
            "[NEGATIVE (0.5742)]\n",
            " 0.5742\n",
            "[NEGATIVE (0.9513)]\n",
            " 0.9513\n",
            "[NEGATIVE (0.9924)]\n",
            " 0.9924\n",
            "[NEGATIVE (0.9994)]\n",
            " 0.9994\n",
            "2021-10-09   1\n",
            "-0.7303666666666666 0.10141624534560165 0.3917906177533448\n",
            "[NEGATIVE (1.0)]\n",
            " 1.0\n",
            "[NEGATIVE (0.9996)]\n",
            " 0.9996\n",
            "[NEGATIVE (0.9854)]\n",
            " 0.9854\n",
            "[NEGATIVE (0.9995)]\n",
            " 0.9995\n",
            "[NEGATIVE (0.9994)]\n",
            " 0.9994\n",
            "[NEGATIVE (1.0)]\n",
            " 1.0\n",
            "[POSITIVE (0.5593)]\n",
            " 0.5593\n",
            "[NEGATIVE (0.7749)]\n",
            " 0.7749\n",
            "[NEGATIVE (0.9997)]\n",
            " 0.9997\n",
            "2021-10-10   2\n",
            "-0.799911111111111 0.1218763092104351 0.3845682124506459\n",
            "------------------------------------------------------------\n",
            "Downloading data from 2021-10-10 06:00:00 to 2021-10-10 07:00:00 for LTCUSDT\n",
            "------------------------------------------------------------\n",
            "Step 0:Downloading data from 2021-10-10 06:00:00 to 2021-10-11 06:00:00\n",
            "\t Downloaded data of len 2 from 2021-10-10 06:00:00 to 2021-10-11 06:00:00\n",
            "------------------------------------------------------------\n",
            "Downloading data from 2021-10-10 06:00:00 to 2021-10-10 07:00:00 for ETHUSDT\n",
            "------------------------------------------------------------\n",
            "Step 0:Downloading data from 2021-10-10 06:00:00 to 2021-10-11 06:00:00\n",
            "\t Downloaded data of len 2 from 2021-10-10 06:00:00 to 2021-10-11 06:00:00\n",
            "------------------------------------------------------------\n",
            "Downloading data from 2021-10-10 06:00:00 to 2021-10-10 07:00:00 for BTCUSDT\n",
            "------------------------------------------------------------\n",
            "Step 0:Downloading data from 2021-10-10 06:00:00 to 2021-10-11 06:00:00\n",
            "\t Downloaded data of len 2 from 2021-10-10 06:00:00 to 2021-10-11 06:00:00\n",
            "enu <enumerate object at 0x7f6721745230>\n",
            "df_csv.columns Index(['open_BTCUSDT', 'high_BTCUSDT', 'low_BTCUSDT', 'close_BTCUSDT',\n",
            "       'volume_BTCUSDT', 'close_LTCUSD', 'volume_LTCUSD', 'close_ETHUSD',\n",
            "       'volume_ETHUSD', 'gnews_flair', 'gnews_tb_polarity',\n",
            "       'gnews_tb_subjectivity', 'gnews_sid_pos', 'gnews_sid_neg',\n",
            "       'gnews_sid_neu', 'gnews_sid_com', 'reddit_flair', 'reddit_tb_polarity',\n",
            "       'reddit_tb_subjectivity', 'reddit_sid_pos', 'reddit_sid_neg',\n",
            "       'reddit_sid_neu', 'reddit_sid_com'],\n",
            "      dtype='object')  len(df_csv.columns)  23\n",
            "23 23 [55607.8, 55646.72, 55606.34, 55615.19, 74.9008, 183.2, 93.797, 3599.33, 501.8367, -0.7999, 0.1219, 0.3846, 0.1079, 0.0507, 0.8412, 0.5431, -0.9886, 0.0, 0.0, 0.091, 0.0, 0.909, 0.2202]\n",
            "index 10-10-2021 07:00\n",
            "df_csv.columns Index(['open_BTCUSDT', 'high_BTCUSDT', 'low_BTCUSDT', 'close_BTCUSDT',\n",
            "       'volume_BTCUSDT', 'close_LTCUSD', 'volume_LTCUSD', 'close_ETHUSD',\n",
            "       'volume_ETHUSD', 'gnews_flair', 'gnews_tb_polarity',\n",
            "       'gnews_tb_subjectivity', 'gnews_sid_pos', 'gnews_sid_neg',\n",
            "       'gnews_sid_neu', 'gnews_sid_com', 'reddit_flair', 'reddit_tb_polarity',\n",
            "       'reddit_tb_subjectivity', 'reddit_sid_pos', 'reddit_sid_neg',\n",
            "       'reddit_sid_neu', 'reddit_sid_com'],\n",
            "      dtype='object')  len(df_csv.columns)  23\n",
            "(33043, 23)\n",
            "open_BTCUSDT              55607.8000\n",
            "high_BTCUSDT              55646.7200\n",
            "low_BTCUSDT               55606.3400\n",
            "close_BTCUSDT             55615.1900\n",
            "volume_BTCUSDT               74.9008\n",
            "close_LTCUSD                183.2000\n",
            "volume_LTCUSD                93.7970\n",
            "close_ETHUSD               3599.3300\n",
            "volume_ETHUSD               501.8367\n",
            "gnews_flair                  -0.7999\n",
            "gnews_tb_polarity             0.1219\n",
            "gnews_tb_subjectivity         0.3846\n",
            "gnews_sid_pos                 0.1079\n",
            "gnews_sid_neg                 0.0507\n",
            "gnews_sid_neu                 0.8412\n",
            "gnews_sid_com                 0.5431\n",
            "reddit_flair                 -0.9886\n",
            "reddit_tb_polarity            0.0000\n",
            "reddit_tb_subjectivity        0.0000\n",
            "reddit_sid_pos                0.0910\n",
            "reddit_sid_neg                0.0000\n",
            "reddit_sid_neu                0.9090\n",
            "reddit_sid_com                0.2202\n",
            "Name: 10-10-2021 07:00, dtype: float64\n",
            "open_BTCUSDT       55607.8000\n",
            "high_BTCUSDT       55646.7200\n",
            "low_BTCUSDT        55606.3400\n",
            "close_BTCUSDT      55615.1900\n",
            "volume_BTCUSDT        74.9008\n",
            "close_LTCUSD         183.2000\n",
            "volume_LTCUSD         93.7970\n",
            "close_ETHUSD        3599.3300\n",
            "volume_ETHUSD        501.8367\n",
            "flair                 -1.7885\n",
            "tb_polarity            0.1219\n",
            "tb_subjectivity        0.3846\n",
            "sid_pos                0.1989\n",
            "sid_neg                0.0507\n",
            "Name: 10-10-2021 07:00, dtype: float64\n",
            "(33043, 14)\n",
            "Number of features 14\n",
            "train: [[0.17383705 0.17285925 0.16959345 ... 0.21776645 0.06050594 0.03008615]\n",
            " [0.16956633 0.16905692 0.16554542 ... 0.39153585 0.15866031 0.1396786 ]\n",
            " [0.16476418 0.1660478  0.16449101 ... 0.40180603 0.09024265 0.05314778]\n",
            " ...\n",
            " [0.51315266 0.51533675 0.50509477 ... 0.5110052  0.17651008 0.04353877]\n",
            " [0.5120015  0.50977623 0.49434954 ... 0.3801381  0.21941146 0.04512922]\n",
            " [0.500989   0.50884116 0.50214493 ... 0.46583563 0.25363967 0.12365805]]\n",
            "Training dataset length  26434\n",
            "Testing dataset length  6609\n",
            "look_back  360\n",
            "total data (33043, 14)   33043\n",
            "(1, 360, 14)\n",
            "[[0.8369786]] (1, 1)\n",
            "prediction: [54561.71069103]\n",
            "Actual 54500.99951398486\n",
            "[54561.71069103]\n",
            "Actual [55615.19004779]\n",
            "{'timestamp': '1633849200', 'predicted': 55615.19, 'last': 54561.7107}\n",
            "2021-10-10 07:01:06.069123 EXECUTED \n",
            "2021-10-10 07:01:06.071004  waiting\n",
            "2021-10-10 07:01:56.096004  waiting\n",
            "2021-10-10 07:02:46.146368  waiting\n",
            "2021-10-10 07:03:36.164492  waiting\n",
            "2021-10-10 07:04:26.170274  waiting\n",
            "2021-10-10 07:05:16.202111  waiting\n",
            "2021-10-10 07:06:06.225247  waiting\n",
            "2021-10-10 07:06:56.230949  waiting\n",
            "2021-10-10 07:07:46.235874  waiting\n",
            "2021-10-10 07:08:36.240164  waiting\n",
            "2021-10-10 07:09:26.247132  waiting\n",
            "2021-10-10 07:10:16.269552  waiting\n",
            "2021-10-10 07:11:06.280376  waiting\n",
            "2021-10-10 07:11:56.312636  waiting\n",
            "2021-10-10 07:12:46.354054  waiting\n",
            "2021-10-10 07:13:36.391984  waiting\n",
            "2021-10-10 07:14:26.442010  waiting\n",
            "2021-10-10 07:15:16.444998  waiting\n",
            "2021-10-10 07:16:06.486802  waiting\n",
            "2021-10-10 07:16:56.538715  waiting\n",
            "2021-10-10 07:17:46.569986  waiting\n",
            "2021-10-10 07:18:36.598219  waiting\n",
            "2021-10-10 07:19:26.607706  waiting\n",
            "2021-10-10 07:20:16.652342  waiting\n",
            "2021-10-10 07:21:06.697937  waiting\n",
            "2021-10-10 07:21:56.748485  waiting\n",
            "2021-10-10 07:22:46.794045  waiting\n",
            "2021-10-10 07:23:36.830633  waiting\n",
            "2021-10-10 07:24:26.842028  waiting\n",
            "2021-10-10 07:25:16.859088  waiting\n",
            "2021-10-10 07:26:06.905388  waiting\n",
            "2021-10-10 07:26:56.922003  waiting\n",
            "2021-10-10 07:27:46.954111  waiting\n",
            "2021-10-10 07:28:37.004534  waiting\n",
            "2021-10-10 07:29:27.005296  waiting\n",
            "2021-10-10 07:30:17.029309  waiting\n",
            "2021-10-10 07:31:07.079636  waiting\n",
            "2021-10-10 07:31:57.111798  waiting\n",
            "2021-10-10 07:32:47.162290  waiting\n",
            "2021-10-10 07:33:37.170014  waiting\n",
            "2021-10-10 07:34:27.180615  waiting\n",
            "2021-10-10 07:35:17.212361  waiting\n",
            "2021-10-10 07:36:07.263427  waiting\n",
            "2021-10-10 07:36:57.311851  waiting\n",
            "2021-10-10 07:37:47.314779  waiting\n",
            "2021-10-10 07:38:37.365772  waiting\n",
            "2021-10-10 07:39:27.411932  waiting\n",
            "2021-10-10 07:40:17.462336  waiting\n",
            "2021-10-10 07:41:07.470359  waiting\n",
            "2021-10-10 07:41:57.519973  waiting\n",
            "2021-10-10 07:42:47.562110  waiting\n",
            "2021-10-10 07:43:37.582331  waiting\n",
            "2021-10-10 07:44:27.631952  waiting\n",
            "2021-10-10 07:45:17.679307  waiting\n",
            "2021-10-10 07:46:07.723602  waiting\n",
            "2021-10-10 07:46:57.771203  waiting\n",
            "2021-10-10 07:47:47.794030  waiting\n",
            "2021-10-10 07:48:37.844404  waiting\n",
            "2021-10-10 07:49:27.882026  waiting\n"
          ]
        }
      ]
    }
  ]
}